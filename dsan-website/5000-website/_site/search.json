[
  {
    "objectID": "data_cleaning/data_cleaning.html",
    "href": "data_cleaning/data_cleaning.html",
    "title": "Data Cleanning",
    "section": "",
    "text": "Data Cleanning\n\nPython\n\nclean the news raw data (gather by Python)\n\n\nExplaination: clean the raw data called iphone_content.txt, and cleaned the stop words and used CountVectorizer\nnewsapi data cleaning code (python)\ncleaned data for newsapi(python)\n\n\n\nR\n\nclean the weibo hot post data (gather by Python)\n\n\nExplaination: clean the raw data called 微博清单.csv, and reordered the dataset, dropped unnecessary columns/rows, and cleaned the text part by segmentation and clear stop words, etc.\nweibo data cleaning code (R)\ncleaned data for weibo(R)"
  },
  {
    "objectID": "ARM/ARM.html",
    "href": "ARM/ARM.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "ARM"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "See the following link for more information about the author: about me\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#topics-introduction-financial-fraud-detection",
    "href": "index.html#topics-introduction-financial-fraud-detection",
    "title": "DSAN-5000: Introduction",
    "section": "1. Topics Introduction: Financial Fraud Detection",
    "text": "1. Topics Introduction: Financial Fraud Detection\n\nsummary: There are many type of financial fraud, credit card fraud, identity theft, account takeover, payment fraud, insurance fraud, money laudering, etc.\nImportance: The financial institution need to collect data, for example, transaction data, account history, customers’ data, etc, to analyze and predict the financial fraud to avoid the financial fraud in the future.\nWhy the reader should continue: there are more and more new fraud methods came up, the past machine learning model might cannot apply to the new fraud methods.\nwhat work had done: The researchers had done many data collection, data preprocessing, and set up machine learning models to do the logistic regression, decision trees and random forest, etc. They also had done so many behavioral analysis to analyze transaction frequency, amount, time of day, etc.\nwhat are the “different points of views”/interpretations in the literature: There are many different points of views related to the financial fraud. From economic Perspective, they may analyze the costs of fraud prevention and the actual costs of fraud. From the data scientists’ perspective, they wish to focus more on using data-driving approaches to prevent fraud. There are also many other perspectives from different researchers.\nwhat exploring: In this area, we need to explore more about the relations between customers’ behavior and the fraud methods\ngoals and hypothesis: clean the data I collect and find some factors that impact the financial fraud and make a fraud detetction model."
  },
  {
    "objectID": "index.html#questions-to-address",
    "href": "index.html#questions-to-address",
    "title": "DSAN-5000: Introduction",
    "section": "2. 10 questions to address",
    "text": "2. 10 questions to address\n\nCan customer reviews and sentiment analysis on social media predict the market success of a new Apple product release?\nHow can we predict whether Apple’s products would be popular in the future?\nHow does Apple’s stock performed?\nWhat kind of electronic products can be seem as a good product to buy?\nWhat kind of models we can used to predict people’s attitude with reviews?\nCan we use machine learning to identify key factors that predict an Apple product’s popularity based on historical data?\nHow does the pricing of Apple products compare with competitors over time, and what is the impact on market share and consumer choice?\nWhat are the trends in iPhone sales over the last decade, and how do they correlate with product release cycles and feature enhancements?\nCan decision tree analysis help in identifying the most significant factors that lead to a successful Apple Store location?\nHow can we eliminate the unnecessary feature for predicting the Apple’s products"
  },
  {
    "objectID": "index.html#realated-articles",
    "href": "index.html#realated-articles",
    "title": "DSAN-5000: Introduction",
    "section": "3. Realated Articles",
    "text": "3. Realated Articles\n\nA comparative study of online consumer reviews of Apple iPhone across Amazon, Twitter and MouthShut platforms1\nAbstract\n\nThe purpose of the paper is to understand if the online consumer reviews differ across the review platforms over the internet. We aim to find the features of the reviews from various platforms and ultimately create a typology of the reviews for those platforms. We apply mixed methods including both quantitative and qualitative techniques to arrive at the conclusion. We find consumers share their views on the highest number of topics in the ecommerce website. Consumers share in-depth views, but on a limited number of topics in other dedicated review platforms. Social media falls somewhere in the middle among these two platforms. While looking into the contents, we could generate themes and meta-themes from these reviews. Based on these facts, we create a typology/ontology for reviews from these platforms and map the motives of reviewers from each platform into the meta-themes identified. Managers can use our findings to boost their online review strategy according to the platform of their interest.\n\n\n\nThe influence of brand popularity, perceived quality, price, and need on purchase intention iPhone products in Purwokerto2\nAbstract\n\nThis study is a survey research that aims to determine and analyze the effect of brand popularity, perceived quality, price, and need on purchase intentions on iPhone products at Purwokerto city. The sample size in this study amounted to 120 using purposive sampling technique. Based on the results of data processing, it is known that brand popularity has no significant and negative effect on purchase intention. Price has a significant and negative effect on purchase intention. Perceived quality and need have a significant and positive effect on purchase intention. The findings in this study are that popularity is not always a consumer factor in generating purchase intentions, while the factor that has the greatest influence on purchase intentions is need.\n\nHIGHLY RECOMMENDED\n\nIt is highly recommended that you build your website using .ipynb files and NOT .qmdfiles\nFunctionally the two formats are basically identical, i.e. they are just Markdown + Code\nHowever there is ONE MAJOR DIFFERENCE, i.e. .ipynb stores the code outputs in the meta-data of the file\n\nThis means you ONLY HAVE TO RUN THE CODE ONCE with .ipynb\n.qmd will run the code every time you build the website, which can be very slow\n\nThere are caching options for .qmd, however, they are “messier” that just using .ipynb\n\nNote: .qmd is fine if there is no code, in which case it is basically just a Markdown file\n\nConverting between the two\n\nYou can switch between the two formats using\nquarto convert clustering.qmd this will output a .ipynb version called clustering.ipynb\nquarto convert eda.ipynb this will output a .qmd version called eda.qmd\n\nYOU CAN RUN R CODE IN VSC WITH .ipynb, see the following link\n\nhttps://saturncloud.io/blog/how-to-use-jupyter-r-kernel-with-visual-studio-code/\n\nIt is possible, but NOT RECOMMENDED, to mix Python and R code in the same file\n\nIMPORTANT ASIDE\n\nA .ipynb file is simply a JSON file with a specialized structural format\nYou can see this by running more eda/eda.ipynb from the command line\nWhich will output the following;\n\n\nTIP FOR MAC USERS\n\ncommand+control+shift+4 is very useful on a mac.\n\nIt takes a screenshot and saves it to the clip-board\n\nThe following VSC extension allows you to paste images from the clip-board with alt+command+v.\n\ntab is your best friend when using the command line, since it does auto-completion\nopen ./path_to_file will open any file or directory from the command line"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "DSAN-5000: Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKundu, Supratim, and Swapnajit Chakraborti. “A Comparative Study of Online Consumer Reviews of Apple IPhone across Amazon, Twitter and MouthShut Platforms.” Electronic Commerce Research, vol. 22, no. 3, 2022, pp. 925–50, https://doi.org/10.1007/s10660-020-09429-w.↩︎\nRizaldi, Herna. ” The influence of brand popularity, perceived quality, price, and need on purchase intention iPhone products in Purwokerto.” Jurnal Akuntansi, Manajemen dan Ekonomi [Online], 24.2 (2022): 14-22. Web. 7 Nov. 2023↩︎"
  },
  {
    "objectID": "decision_trees/decision_trees.html",
    "href": "decision_trees/decision_trees.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Decision Trees"
  },
  {
    "objectID": "data_gathering/data_gathering.html",
    "href": "data_gathering/data_gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "Data Gathering\n\nPython\n\nGathering data of hot post from Weibo\n\n\nExplaination: crawl data from Chinese social platform, Weibo, to catch some hot posts about people’s reaction to the new publishion of Apple product\nweibo data gathering code (python)\nraw data for weibo(python)\nsample raw data \n\n\nGathering data of news text\n\n\nExaplaination: crawl data by using NewsAPI and try to extract some key words from the news, and see what’s the attitude of media towards the Iphone.\nNews data gathering code (python)\nraw data for newsAPI(python)\nsample raw data \n\n\n\nR\n\nGathering data of news from website\n\n\nExaplaination: craw text by using Rvest and get content from the news website.\nNews webpage data gathering code (R)\nraw data for news webpage(R)\nsample raw data"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "In this part, I will use visualization tools to help the audience know more about my project, which will include text clouds, statistic distribution, correlation heatmap, etc. This will help the audience to understand the general partterns in the data, and analysis the data.\n\n\n\n\n\n\n\nIn this text dataset, the type for this dataset is categorical data.\n\n\n\n\n\ncode in the news_cleanning.ipynb\n\n\n\n\nExplaination: After I cleaned the data with Numpy and Pandas package as I needed, I use the Wordcloud package to output the most frequent appeared words in those articles.\n\n\n\ntext cloud for news articles\n\n\n\n\n\n\n\n\nExplaination: I cleared out all the stopwords as needed, and get the most frequent words out to see the correlation between those key words. We can see that the color of words more close to white is more positive related, and the color more close to black is negative related. I used\n\n\n\ncorrelation heatmap for news articles\n\n\n\n\n\n\n\nfrom the analysis from above, we can see most of the key words from the news are positive or natural, which means, the public has postive attitude towards apple products.\n\n\n\n\n\n\n\n\nFor this dataset, it’s all numerical data. The open/close price are closely relate to each other\ncode in the code/EDA/apple_stock.ipynb file\n\n\n\n\n\n\n\n\nstatistics for apple stock\n\n\nAs shwon in the table, we can the standard deviation for the apple stock is around 19, it show a floating with the apple stock within days.\nhowever the max and min are similar for each open/close price, shows a stable status for the apple stock.\n\n\n\n\n\n\n\nExplaination: I collected the dataset of stock price from Yahoo! finance, and used Matplotlib, plotly and seaborn to plot the stock price in line plot with the open price and the date.\n\n\n\nopen price for apple stock\n\n\nWe can see from this plot that the price is in the increasing trend, which means the apple stock are more popular.\n\n\n\n\n\nExplaination: I collected the dataset of stock price from Yahoo! finance, and used Matplotlib, plotly and seaborn to plot the stock trading volume in each date in line plot\n\n\n\nopen price for apple stock\n\n\nAs shown, we can see the trading volume are decreasing during the time, which may indicate that the apple’s stock are less people buying, and it may related to the price.\n\n\n\n\n\n\n\n\n\nOutliers for apple stock\n\n\nAs shown, there’s no outliers for this stock, so, every points can be inclued in evaluating this data.\n\n\n\n\n\nfrom the analysis from above, we can see the apple stock price is still increasing, and seems still be popular to the public. This may indicate the Apple product is still popular in the market.\n\n\n\n\n\n\n\n\nFor this dataset, below are their data type.\n\n\n\nMobile Phone data type\n\n\nThe launch price, camera, selfie, audio, display, battery ratings are all related to the reank.\n\n\n\n\n\n\n\n\n\n\nThe relation of Launch Price and Buing Intent\n\n\nused Seaborn\nIt shows that the launch price actually determined whether willing to buy. When the price is around 750-1250 dollars, people are very willing to buy.\n\n\n\n\n\n\n\n\nThe relation between Buying Intent and Camera Rating\n\n\nused Seaborn\nIt also shows people willingness to buy are connected to the camera’s quality. the camera score that people very willing to buy is around 130-160 range.\n\n\n\n\n\n\n\n\nTop 50 Ranking Brands\n\n\nused Matplotlib\nAs shown in the graph, there are 20% Top 50 ranking phones are apples. Which shows, Apple is still one of the most popular brands in the market.\n\n\n\n\n\n\n\n\n\nThe correlation between features\n\n\nUsed heatmap\nIt semmed that camera and launch price are correlated to each other. And the Launch price camera score and audio score are negatively correlated to each other.\n\n\n\n\n\n\n\n\nstatistics for mobile phone\n\n\nthe mean Launch price is around $773.5 and the mean score for selfie is around 130, which is obviously higher than others (this is becuase I used mean to insert missing value)\n\n\n\n\n\nfrom the analysis we can analyze what kind of features would be popular around customers. This can be used for analysis whether apple would be best product among choices. It still prove that Apple is popular among customers.\n\n\n\n\n\n\n\n\nThis is a text record data, and it is a categorical data.\n\n\n\n\n\nfrequency distribution\n\n\n\nIphone 11 ratings\n\n\nUsed Seabron\nFrom the graph, we can see most people give 5.0 ratings, which shows that most customers satisfied the apple products\n\n\n\n\n\n\n\n\nstatistics for Ipohone11 ratings\n\n\nThe mean rating is 4.48 which is very high, shows that everyone like iphone 11\n\n\n\n\n\nfrom the analysis we can analyze that most customers satisfied apple’s products, so apple may be still popular in the market."
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Methods & Codes",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf=pd.read_csv(\"../../../data/01-modified-data/after_clean_mobile_phone_rating.csv\")"
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html",
    "href": "dimensionality_reduction/dimensionality_reduction.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Dimensionality Reduction"
  },
  {
    "objectID": "conclusion/conclusion.html",
    "href": "conclusion/conclusion.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Conclusion"
  },
  {
    "objectID": "eda/eda.html#introduction-to-eda",
    "href": "eda/eda.html#introduction-to-eda",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "In this part, I will use visualization tools to help the audience know more about my project, which will include text clouds, statistic distribution, correlation heatmap, etc. This will help the audience to understand the general partterns in the data, and analysis the data."
  },
  {
    "objectID": "eda/eda.html#news-articles-dataset",
    "href": "eda/eda.html#news-articles-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "In this text dataset, the type for this dataset is categorical data.\n\n\n\n\n\ncode in the news_cleanning.ipynb\n\n\n\n\nExplaination: After I cleaned the data with Numpy and Pandas package as I needed, I use the Wordcloud package to output the most frequent appeared words in those articles.\n\n\n\ntext cloud for news articles\n\n\n\n\n\n\n\n\nExplaination: I cleared out all the stopwords as needed, and get the most frequent words out to see the correlation between those key words. We can see that the color of words more close to white is more positive related, and the color more close to black is negative related. I used\n\n\n\ncorrelation heatmap for news articles\n\n\n\n\n\n\n\nfrom the analysis from above, we can see most of the key words from the news are positive or natural, which means, the public has postive attitude towards apple products."
  },
  {
    "objectID": "eda/eda.html#apple-stock-dataset",
    "href": "eda/eda.html#apple-stock-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "For this dataset, it’s all numerical data. The open/close price are closely relate to each other\ncode in the code/EDA/apple_stock.ipynb file\n\n\n\n\n\n\n\n\nstatistics for apple stock\n\n\nAs shwon in the table, we can the standard deviation for the apple stock is around 19, it show a floating with the apple stock within days.\nhowever the max and min are similar for each open/close price, shows a stable status for the apple stock.\n\n\n\n\n\n\n\nExplaination: I collected the dataset of stock price from Yahoo! finance, and used Matplotlib, plotly and seaborn to plot the stock price in line plot with the open price and the date.\n\n\n\nopen price for apple stock\n\n\nWe can see from this plot that the price is in the increasing trend, which means the apple stock are more popular.\n\n\n\n\n\nExplaination: I collected the dataset of stock price from Yahoo! finance, and used Matplotlib, plotly and seaborn to plot the stock trading volume in each date in line plot\n\n\n\nopen price for apple stock\n\n\nAs shown, we can see the trading volume are decreasing during the time, which may indicate that the apple’s stock are less people buying, and it may related to the price.\n\n\n\n\n\n\n\n\n\nOutliers for apple stock\n\n\nAs shown, there’s no outliers for this stock, so, every points can be inclued in evaluating this data.\n\n\n\n\n\nfrom the analysis from above, we can see the apple stock price is still increasing, and seems still be popular to the public. This may indicate the Apple product is still popular in the market."
  },
  {
    "objectID": "eda/eda.html#iphone_11_review_dataset",
    "href": "eda/eda.html#iphone_11_review_dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "This is a text record data, and it is a categorical data.\n\n\n\n\n\nfrequency distribution\n\n\n\nIphone 11 ratings\n\n\nUsed Seabron\nFrom the graph, we can see most people give 5.0 ratings, which shows that most customers satisfied the apple products\n\n\n\n\n\n\n\n\nstatistics for Ipohone11 ratings\n\n\nThe mean rating is 4.48 which is very high, shows that everyone like iphone 11\n\n\n\n\n\nfrom the analysis we can analyze that most customers satisfied apple’s products, so apple may be still popular in the market."
  },
  {
    "objectID": "eda/eda.html#mobile_phone_rating-dataset",
    "href": "eda/eda.html#mobile_phone_rating-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "For this dataset, below are their data type.\n\n\n\nMobile Phone data type\n\n\nThe launch price, camera, selfie, audio, display, battery ratings are all related to the reank.\n\n\n\n\n\n\n\n\n\n\nThe relation of Launch Price and Buing Intent\n\n\nused Seaborn\nIt shows that the launch price actually determined whether willing to buy. When the price is around 750-1250 dollars, people are very willing to buy.\n\n\n\n\n\n\n\n\nThe relation between Buying Intent and Camera Rating\n\n\nused Seaborn\nIt also shows people willingness to buy are connected to the camera’s quality. the camera score that people very willing to buy is around 130-160 range.\n\n\n\n\n\n\n\n\nTop 50 Ranking Brands\n\n\nused Matplotlib\nAs shown in the graph, there are 20% Top 50 ranking phones are apples. Which shows, Apple is still one of the most popular brands in the market.\n\n\n\n\n\n\n\n\n\nThe correlation between features\n\n\nUsed heatmap\nIt semmed that camera and launch price are correlated to each other. And the Launch price camera score and audio score are negatively correlated to each other.\n\n\n\n\n\n\n\n\nstatistics for mobile phone\n\n\nthe mean Launch price is around $773.5 and the mean score for selfie is around 130, which is obviously higher than others (this is becuase I used mean to insert missing value)\n\n\n\n\n\nfrom the analysis we can analyze what kind of features would be popular around customers. This can be used for analysis whether apple would be best product among choices. It still prove that Apple is popular among customers."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html",
    "href": "Naive_Bayes/Naive_Bayes.html",
    "title": "Introduction to Naive Bayes:",
    "section": "",
    "text": "The process of training the Naive Bayes algorithm is actually the process of solving each a priori probability P(Ai) and conditional probability P(Bj|Ai), and then in the prediction and classification process, it is only necessary to substitute each probability value obtained during the training process into the Bayesian formula, so as to obtain the relative probability of the results of the classification labels under the current features. The one with higher relative probability is naturally used as the predicted label.\n\n\n\n\n\n\\(P(Y|X) = (P(X|Y) * P(Y))/P(X)\\)\nP(A|B) is called the a posteriori probability, for the target to be found\nP(A) is the a priori probability, which can be statistically derived from a large amount of data, or empirically provided when the amount of data is small.\nP(B|A) is the conditional probability, which can be statistically derived from a large amount of data, and is usually given by great likelihood estimation, and is actually also the a priori probability\n\n\n\n\n\nGaussian Naive Bayes: This variant of Naive Bayes is best suited for datasets where the features are continuous and follow a Gaussian or normal distribution. It assigns a Gaussian distribution to each class and predicts the class of a new data point based on these distributions. Common applications include problems that involve continuous variables, as is often found in real-world datasets.\nMultinomial Naive Bayes: This type excels with discrete data and finds frequent application in text categorization tasks, where text is converted into word frequency vectors. However, it can also work with tf-idf vectors. It computes the conditional probability of a word given a class, making it a good choice for problems involving discrete data.\nBernoulli Naive Bayes: This method is ideal for binary or boolean features and is typically used in text classification tasks where the presence or absence of a word is more important than its frequency. It assumes that all our features are binary-valued and models the input data with a multivariate Bernoulli distribution.\n\n\n\n\nI will use Naive Bayes Algorithmn to train 2 datasets, the objective for using this is for accurately predicting the lebels for my datasets:\n\nMobile phone rating: This is a record dataset. There are features like mobiles camera scores, audio scores, display scores, etc, and those related to the customers’ willingness to buy this phone or not. The main goal for training this dataset is for repeict customer’s willingness to buy a phone or not depends on those feature scores.\nIphone 11 reviews: This is a text dataset. The text are customers reviews. The main goal for this data training is by through the reviews, predict the attitutes of the customers."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#introduction-to-naive-bayes",
    "href": "Naive_Bayes/Naive_Bayes.html#introduction-to-naive-bayes",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The process of training the Naive Bayes algorithm is actually the process of solving each a priori probability P(Ai) and conditional probability P(Bj|Ai), and then in the prediction and classification process, it is only necessary to substitute each probability value obtained during the training process into the Bayesian formula, so as to obtain the relative probability of the results of the classification labels under the current features. The one with higher relative probability is naturally used as the predicted label.\n\n\n\n\n\n\\(P(Y|X) = (P(X|Y) * P(Y))/P(X)\\)\nP(A|B) is called the a posteriori probability, for the target to be found\nP(A) is the a priori probability, which can be statistically derived from a large amount of data, or empirically provided when the amount of data is small.\nP(B|A) is the conditional probability, which can be statistically derived from a large amount of data, and is usually given by great likelihood estimation, and is actually also the a priori probability"
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#prepare-dataset",
    "href": "Naive_Bayes/Naive_Bayes.html#prepare-dataset",
    "title": "Introduction to Naive Bayes:",
    "section": "Prepare Dataset",
    "text": "Prepare Dataset\n\ncreate labels\nI create labels for the mobile phone by its rankings\n(code in codes/data-cleaning/mobile_phone_rating.ipynb).\n\nThe top 50 ranking phones is Very Willing\nThe 51-100 ranking phones is Moderately Willing\nThe 101-last ranking ophones is Not Willing\n\n\n\nSplit data\n(code in codes/naive_bayes/mobile_phone_rating_training)\n\nI split data into traning set 70%, validation set 15%, and test set 15%.\n70% in training will increase the accuracy for the model learning. And the rest each 15% for better identify the training results."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#feature-selection",
    "href": "Naive_Bayes/Naive_Bayes.html#feature-selection",
    "title": "Introduction to Naive Bayes:",
    "section": "Feature Selection",
    "text": "Feature Selection\n(code in codes/naive_bayes/mobile_phone_rating_training)\nAccording to the number of features, I found out it nearly has no impact on the trainig. - \nThus I chose to use Vairance Thredhold for this data, after the selection, I find out the optimal thredhold. It turns out the optimal thredhold is 26.51, and the accuracy is around 88%."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#naive-bayes-with-labeled-record-data",
    "href": "Naive_Bayes/Naive_Bayes.html#naive-bayes-with-labeled-record-data",
    "title": "Introduction to Naive Bayes:",
    "section": "Naive Bayes with Labeled Record Data",
    "text": "Naive Bayes with Labeled Record Data\n(code in codes/naive_bayes/mobile_phone_rating_training)\n\nHowever, after I tried several times for selecting the vairance thredhold, becuase the feature selection process above elimiating some cornor cases, but there are not that many features in my model, if i eliminating them, it would decrease my model accuracy. So after all, I found the optimal thredhold should be 45, so I choose to use that for my model.\nIn that way, the ‘Selfie’ column will be deducted, which make sense because when I did data cleaning process, there are many nulls and I use the mean to fill null.\n\n\nfinal result\n\nEvaluation Matrices\n\n\n\n\nEvaluation Metrics\n\n\nAs shown above, the accuracy of my model is 91.3% and which is very high, shows it’s a good model.\nPrecision is comparely low for the “not will”, but it’s ok, since the number of that labels are not too many.\nF-1 score and recall performs well.\nMacro Avg: Precision: 86%, Recall: 94%, F1-Score: 89% are all very high schore, shows it’s a good model to fit in.\nWeighted Avg: Precision: 93%, Recall: 91% , F1-Score: 92%, shows it’s a good model to fit in.\nIt did not show any overfitting or underfitting,since the test scores perform well.\n\n\n\nconfusion matrices\n\n\n\n\nConfusion Metrics\n\n\nAs shown above, the accuracy for predicting labels are very high. Only 2 labels are inaccurately predicted."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#conclusion",
    "href": "Naive_Bayes/Naive_Bayes.html#conclusion",
    "title": "Introduction to Naive Bayes:",
    "section": "Conclusion",
    "text": "Conclusion\nThe model predict my dataset well. After he variance Thredhold feature selection, the accuracy increased from about 88% to 91.3%. Also, the model did not show overfitting/underfitting because of its traning accuracy and test accuracy are all around 90%. Thus this model can well predict how those scores impact people’s buying willingness."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#prepare-dataset-1",
    "href": "Naive_Bayes/Naive_Bayes.html#prepare-dataset-1",
    "title": "Introduction to Naive Bayes:",
    "section": "Prepare Dataset",
    "text": "Prepare Dataset\n\ncreate labels\nI create labels for the mobile phone by its ratings\n(code in codes/naive_bayes/iphone_11_review_training.ipynb).\n\nThe top 50 ranking phones is Very Willing\nThe 51-100 ranking phones is Moderately Willing\nThe 101-last ranking ophones is Not Willing\n\n\n\nSplit data\n(code in codes/naive_bayes/iphone_11_review_training.ipynb).\n\nI split data into traning set 80%, and test set 20%.\n80% in training will increase the accuracy for the text model learning. Also, 20% of test dataset is enough for giving good results."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#feature-selection-1",
    "href": "Naive_Bayes/Naive_Bayes.html#feature-selection-1",
    "title": "Introduction to Naive Bayes:",
    "section": "Feature Selection",
    "text": "Feature Selection\n(code in codes/naive_bayes/iphone_11_review_training.ipynb)\n\n\n\n\nfeature selection\n\n\nAs shown in the graph, when the number of features close to 600-700, the test and training accuracy score did not improve. And after that, the model indicate it’s overfitting because the taining accuracy is high but test accuracy is not that hight. So our opimal feature selection should around 600-700.\n\n\n\nrunning time\n\n\nAs shown in the graph, when the number of features close to 1000 the running time increased a lot\n\nBased on those observation, and also from the accuracy scores of each number of features, I get the optimal number of features are 640. With this feature number, the accuracy and running time are optimal."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#naive-bayes-with-labeled-text-data",
    "href": "Naive_Bayes/Naive_Bayes.html#naive-bayes-with-labeled-text-data",
    "title": "Introduction to Naive Bayes:",
    "section": "Naive Bayes with Labeled Text Data",
    "text": "Naive Bayes with Labeled Text Data\n(code in codes/naive_bayes/iphone_11_review_training.ipynb)\n\n\n\n\nEvaluation Metrics\n\n\nAs shown above, the accuracy of my model is 90.11% and which is very high, shows it’s a good model.\nMacro Avg: Precision: 65%, Recall: 58%, F1-Score: 61%, those scores are comparedly not perform very well, it is because some low performance for the natural(0) class, this is also make sense, because there are not many data related to that class.\nWeighted Avg: Precision: 89%, Recall: 90% , F1-Score: 89%, shows it’s a good model to fit.\n\n\nconfusion matrices\n\n\n\n\nConfusion Metrics\n\n\nAs shown above, the accuracy for predicting right labels are very high.\nIn total, 903 data labels were predicted accurately. The model predited well in the positive and negative classes, but did not perform well in the natural class. This is because there are not so many data of natural."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#conclusion-1",
    "href": "Naive_Bayes/Naive_Bayes.html#conclusion-1",
    "title": "Introduction to Naive Bayes:",
    "section": "conclusion",
    "text": "conclusion\nThe model predict my dataset well. After the number of features selection method, the accuracy can reach 90.11%. However, because the insufficient of data related to natural label class, the prediction to this class did not perform so well as expected.\nAlso, the model did not show overfitting/underfitting because of its traning accuracy and test accuracy are all around 90%.\nThus, overall the model performed well, and it can based on the text reviews predict poeple’s attitude towards iphone 11 products."
  },
  {
    "objectID": "index.html#topics-introduction-iphones-popularity-in-the-markey",
    "href": "index.html#topics-introduction-iphones-popularity-in-the-markey",
    "title": "DSAN-5000: Introduction",
    "section": "1. Topics Introduction: iPhone’s popularity in the markey",
    "text": "1. Topics Introduction: iPhone’s popularity in the markey\n\nsummary: Over the decades, Apple’s products are always the most popular products in the markets. However, with more and more new products introduced to the market. Is Apple products, especially iphone, are still popular in the market?\nImportance: The iPhone has significantly influenced technology and culture since its introduction in 2007. It has influenced and impacted a genration, thus everyone curious about how apple performed in the recent years. Its role in shaping consumer behaviors, communication norms, and mobile technology makes it a rich subject for study.\nWhy the reader should continue: The choices and preferences of iPhone users can reflect broader trends in consumer behavior. Consumers can pick better products through this research topic and see how consumers react to Apple’s products.\nwhat work had done: The researchers had done many data collection, data preprocessing, and set up machine learning models to do the logistic regression, decision trees and random forest, etc. They also had done so many behavioral analysis to analyze the popularity trend for electronic products.\nwhat are the “different points of views”/interpretations in the literature: Some literature emphasizes Apple’s commitment to innovation and high-quality products as a key factor in its popularity. This view holds that the company’s focus on design, user experience, and robust performance drives consumer preference. Some authors also discuss Apple’s cultural impact, suggesting that its products have become more than just technology but a part of modern culture.\nwhat exploring:\n\nWe can explore how people evaluate an electronic products as a good products.\nWe can explore the stock market for the Apple Products and how it stock prices changes\nWe can collect customers’ review/news on Apple products, to see how everyone like apple’s products.\n\ngoals and hypothesis: clean the data I collect and find some factors that impact the financial fraud and make a fraud detetction model."
  },
  {
    "objectID": "clustering/clustering1.html",
    "href": "clustering/clustering1.html",
    "title": "Clustering",
    "section": "",
    "text": "In my Mobile Phone Rating Dataset, the features are including the ratings about the function of the mobile phones. There are 6 features, Launch Price, camera rating, selfie rating, audio rating, display rating, and battery rating.\nThe purpose of this clustering analysis is for doing the unsupervised learning to find out the possible clusters of the dataset and what is the optimal cluster number"
  },
  {
    "objectID": "clustering/clustering.html#k-means",
    "href": "clustering/clustering.html#k-means",
    "title": "Methods & Codes",
    "section": "K means",
    "text": "K means\n\n# for k means clustering we will use the elbow method to find the optimal number of clusters. we will use the inertia_ attribute to find the sum of squared distances of samples to their closest cluster center. we will use the range of 1 to 10 clusters. plot the inertia_ values for each number of clusters. make sure to save it in a dataframe and plot it using matplotlib.\ninertia = []\ndistortions = []\nfor num_clusters in range(1, 11):\n    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n    kmeans.fit(X_normalized)\n    inertia.append(kmeans.inertia_)\n    distortions.append(sum(np.min(cdist(X_normalized,kmeans.cluster_centers_, 'euclidean'),axis=1)) /X_normalized.shape[0])\n\ndf_inertia = pd.DataFrame({'Number of Clusters': range(1, 11), 'Inertia': inertia})\ndf_distortions=pd.DataFrame({'Number of Clusters': range(1, 11), 'distortions': distortions})\n\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n# plot distortion and inertia for kmeans, you can either plot them seperately or use fig, ax = plt.subplots(1, 2) to plot them in the same figure. Suggest the optimal number of clusters based on the plot.\nplt.plot(df_inertia['Number of Clusters'], df_inertia['Inertia'], marker='o', linestyle='-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal Number of Clusters')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nplt.plot(df_inertia['Number of Clusters'], df_distortions['distortions'], marker='o', linestyle='-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Distortions')\nplt.title('Elbow Method for Optimal Number of Clusters')\nplt.grid(True)\nplt.show()\n\n\n\n\nBy using K means algorithmn, we can see the inertia and distortion with different number of clusters by using hyper-parameter elbow methods. From the graphs above, it seemed that 2 is the optimal number of clusters for me dataset. However, it’s not that obious elbow can be shown from the graph."
  },
  {
    "objectID": "clustering/Results.html",
    "href": "clustering/Results.html",
    "title": "Results Analysis",
    "section": "",
    "text": "Optimal Clusters\n\n\nK-means: 2\nDBSCAN : 4\nAgglomerative Hierarchy: 2\nMeanshift : 4\nBirch : 3\n\n\nAnalysis I think the better method would be Agglomerative Hierarchy for the following reasons.\n\n\nIt’s easier for me to use, because I do not need to identify the k clusters before using it. It would automatically give me the optimal result.\nSince my dataset is not that big, so using agglomerative is more effective to my dataset.\nIt can also make me easily see the hierarchy relationships with my dataset.\nThe final result of the optimal clusters are 2 is making sense, because in my original dataset, I labeled them into 3 clusters and it turned out that for the “nature” cluster, there’s not that many dataset belongs to this cluster.\nCompared to K-means, as shown in the graphs of K-means, the elbow is not that obvious for me. Compared to DBSCAN and Meansift’s result, I think 4 clusters would make the clusters contained not enough data points.\nAfter observations, I believe the higher of the price and the other function rating features would be seperated into a cluster. And the lower of the price and the other function rating features would be seperated into another cluster."
  },
  {
    "objectID": "clustering/Results.html#different-methods-comparing",
    "href": "clustering/Results.html#different-methods-comparing",
    "title": "Results Analysis",
    "section": "",
    "text": "Optimal Clusters\n\n\nK-means: 2\nDBSCAN : 4\nAgglomerative Hierarchy: 2\nMeanshift : 4\nBirch : 3\n\n\nAnalysis I think the better method would be Agglomerative Hierarchy for the following reasons.\n\n\nIt’s easier for me to use, because I do not need to identify the k clusters before using it. It would automatically give me the optimal result.\nSince my dataset is not that big, so using agglomerative is more effective to my dataset.\nIt can also make me easily see the hierarchy relationships with my dataset.\nThe final result of the optimal clusters are 2 is making sense, because in my original dataset, I labeled them into 3 clusters and it turned out that for the “nature” cluster, there’s not that many dataset belongs to this cluster.\nCompared to K-means, as shown in the graphs of K-means, the elbow is not that obvious for me. Compared to DBSCAN and Meansift’s result, I think 4 clusters would make the clusters contained not enough data points.\nAfter observations, I believe the higher of the price and the other function rating features would be seperated into a cluster. And the lower of the price and the other function rating features would be seperated into another cluster."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html",
    "href": "dimensionality_reduction/reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "In this tab, I will try to use PCA and t-SNE method to reduce the dimension for better analyzing the mobile phone dataset.\nIn this part, the library will include json, numpy, pandas, matplotlib, scikit-learn, etc."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#project-proposal",
    "href": "dimensionality_reduction/reduction.html#project-proposal",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "In this tab, I will try to use PCA and t-SNE method to reduce the dimension for better analyzing the mobile phone dataset.\nIn this part, the library will include json, numpy, pandas, matplotlib, scikit-learn, etc."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#code",
    "href": "dimensionality_reduction/reduction.html#code",
    "title": "Dimensionality Reduction",
    "section": "Code",
    "text": "Code\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\ndf=pd.read_csv(\"../../../data/01-modified-data/after_clean_mobile_phone_rating.csv\")\n\n\nx=df[['Launch Price','CAMERA','SELFIE','AUDIO','DISPLAY','BATTERY']]\ny=df[['buying_intent']]\nscaler = StandardScaler()\nX = scaler.fit_transform(x)\n\n\nDimensionality Reduction with PCA\n\n# EIGEN VALUES/VECTOR\nfrom numpy import linalg as LA\n# w, v1 = LA.eig(cov)\nw, v1 = LA.eig(np.cov(X.T))\nprint(\"\\nCOV EIGENVALUES:\",w)\nprint(\"COV EIGENVECTORS (across rows):\")\nprint(v1.T)\n\n\nCOV EIGENVALUES: [2.46422101 1.21865397 0.24126397 0.958379   0.63183561 0.52512012]\nCOV EIGENVECTORS (across rows):\n[[ 0.50024701  0.53945186  0.20733577  0.42438368  0.44295301 -0.19861426]\n [ 0.39766903  0.19014289 -0.67810966 -0.25218719 -0.31386294 -0.42867702]\n [-0.69792826  0.69118266 -0.15187461 -0.05535362  0.03015126 -0.09013136]\n [ 0.21875576  0.21730996 -0.34121012 -0.27020298  0.26679962  0.8026869 ]\n [-0.19511955 -0.26042618 -0.57511145  0.73310744  0.13997149  0.07946605]\n [ 0.13631836  0.2829461   0.16412113  0.3778623  -0.78332594  0.34357475]]\n\n\n\n# PCA CALCULATION\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=6)\npca.fit(X)\nprint('\\nPCA')\nprint(pca.components_)\n\n\nPCA\n[[-0.50024701 -0.53945186 -0.20733577 -0.42438368 -0.44295301  0.19861426]\n [ 0.39766903  0.19014289 -0.67810966 -0.25218719 -0.31386294 -0.42867702]\n [ 0.21875576  0.21730996 -0.34121012 -0.27020298  0.26679962  0.8026869 ]\n [-0.19511955 -0.26042618 -0.57511145  0.73310744  0.13997149  0.07946605]\n [ 0.13631836  0.2829461   0.16412113  0.3778623  -0.78332594  0.34357475]\n [ 0.69792826 -0.69118266  0.15187461  0.05535362 -0.03015126  0.09013136]]\n\n\n\n# # PLOT\nv2=pca.components_\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X[:,0],X[:,1],X[:,2],marker=\".\", cmap=\"viridis\")\nv1=v1*1000\nv2=v2*1000\n\nax.quiver(0,0,0,v1[0,0],v1[1,0],v1[2,0])\nax.quiver(0,0,0,v1[0,1],v1[1,1],v1[2,1])\nax.quiver(0,0,0,v1[0,2],v1[1,2],v1[2,2])\n\nax.quiver(0,0,0,v2[0,0],v2[1,0],v2[2,0])\nax.quiver(0,0,0,v2[0,1],v2[1,1],v2[2,1])\nax.quiver(0,0,0,v2[0,2],v2[1,2],v2[2,2])\nplt.show()\n\n/var/folders/rx/nxz5w_293m7dsx3fbcd187380000gn/T/ipykernel_26861/2939638508.py:5: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  ax.scatter(X[:,0],X[:,1],X[:,2],marker=\".\", cmap=\"viridis\")\n\n\n\n\n\n\nplt.figure(figsize=(8,4))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='95% explained variance')\ncum_explained=np.cumsum(pca.explained_variance_ratio_)\noptimal_components = np.where(cum_explained &gt;= 0.95)[0][0]+1\nplt.axvline(x=optimal_components, color='red', linestyle='--', linewidth=2, label=f'Optimal components: {optimal_components}')\nplt.show()\n\n\n\n\nAs shown, the number of components that can make 95% cummulative explained variance is 5.\n\n\nDimensionality Reduction with t-SNE\n\nfrom sklearn.manifold import TSNE\n\n\n# 2D\ntsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000)\nX_tsne = tsne.fit_transform(X)\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\nplt.show()\n\n\n\n\n\n# 3D\ntsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2])\n\nax.set_xlabel('t-SNE Feature 1')\nax.set_ylabel('t-SNE Feature 2')\nax.set_zlabel('t-SNE Feature 3')\nplt.title('3D t-SNE Visualization')\nplt.show()\n\n\n\n\n\n#parameter tuning for t-SNE (perplexity)\nperplexities = [5, 25, 50]\nlearning_rates = [10, 200, 500]\n\nfig, axs = plt.subplots(len(perplexities), len(learning_rates), figsize=(15, 10))\n\nfor i, perplexity in enumerate(perplexities):\n    for j, learning_rate in enumerate(learning_rates):\n        tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=1000)\n        X_tsne = tsne.fit_transform(X)\n        axs[i, j].scatter(X_tsne[:, 0], X_tsne[:, 1])\n        axs[i, j].set_title(f'Perplexity: {perplexity}, Learning Rate: {learning_rate}')\n\nplt.show()\n\n\n\n\nAs shown in the graph, the lower perplexity the better for analyzing the data. Compared to PCA, I think t-SNE is less practical to this dataset than PCA."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#project-report---evaluation-and-comparasion",
    "href": "dimensionality_reduction/reduction.html#project-report---evaluation-and-comparasion",
    "title": "Dimensionality Reduction",
    "section": "Project Report - Evaluation and comparasion",
    "text": "Project Report - Evaluation and comparasion\n\nPCA\nStrength:\n\nPCA is better to be used in the small and simple constructed dataset.\nIt reduce the noise in the dataset\npreserve the global structure\nworks well with linear dimensionality reduction\n\nWeakness:\n\ndoes not involve hyperparameter tuning\nOversimplification of the data\n\n\n\nt-SNE\nStrength:\n\ninvolve hyperparameters tuning\nPreserves Local and Global Structure\n\nWeakness:\n\nwhen we are trying to reducing the dimension, the distance between points in low dimension are actually not match with the actual distance of the high dimension\nIn the high dimension, if the distance between points are large, but when we convert to low dimension, the distance may be smaller.\n\n\n\nConclusion\nIf the dataset is more linear and dataset size is small, using PCA would be better. If the dataset is not linear and the dataset size is large, using t-SNE would be better.\nI think PCA is better to use in this dataset because as shown in the t-SNE graphs, I don’t think the pattern are easily shown or analyized but PCA’s graph are more easier to see and analyze. Also, my dataset is not large, so it’s better to use PCA over t-SNE."
  },
  {
    "objectID": "clustering/clustering1.html#footnotes",
    "href": "clustering/clustering1.html#footnotes",
    "title": "Clustering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“K-Means Clustering.” Wikipedia, Wikimedia Foundation, 12 Oct. 2023, en.wikipedia.org/wiki/K-means_clustering.↩︎\n“DBSCAN.” Wikipedia, Wikimedia Foundation, 27 Oct. 2023, en.wikipedia.org/wiki/DBSCAN.↩︎\n“Hierarchical Clustering.” Wikipedia, Wikimedia Foundation, 10 Oct. 2023, en.wikipedia.org/wiki/Hierarchical_clustering.↩︎"
  }
]