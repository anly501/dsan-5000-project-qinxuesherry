[
  {
    "objectID": "data_cleaning/data_cleaning.html",
    "href": "data_cleaning/data_cleaning.html",
    "title": "Data Cleanning",
    "section": "",
    "text": "The purpose of data cleansing is to remove data noise points by eliminating erroneous, duplicate parts of the data. In this part, I will first do a statistical analysis to identify the dataset’s mean, variance, frequency, etc. This step lets us find the datasets’ noise points and clean them as needed. For the record data, most data cleaning processes will deal with the abnormal values, fill null values, remove missing values, etc. For the text data, most data cleaning processes will involve cleaning the stop words, removing white spaces, etc."
  },
  {
    "objectID": "ARM/ARM.html",
    "href": "ARM/ARM.html",
    "title": "ARM",
    "section": "",
    "text": "Association rule mining (ARM) can be used in market analysis to determine what products, given their previous purchases, consumers will also buy. In order to examine consumer purchasing patterns For example, the ARM test can be run on an e-commerce dataset containing electronic transactions of various products by customers. i.e., people are more likely to buy a particular product later on if they purchase it first.\nThe antecedents are the feature data X. X is the description of the product that the customer purchased in this collection. Finding consequents (Y) is the first step in determining how likely it is that a consumer will buy product X and then purchase product Y based on their confidence and lift."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "See the following link for more information about me: about me"
  },
  {
    "objectID": "index.html#topics-introduction-financial-fraud-detection",
    "href": "index.html#topics-introduction-financial-fraud-detection",
    "title": "DSAN-5000: Introduction",
    "section": "1. Topics Introduction: Financial Fraud Detection",
    "text": "1. Topics Introduction: Financial Fraud Detection\n\nsummary: There are many type of financial fraud, credit card fraud, identity theft, account takeover, payment fraud, insurance fraud, money laudering, etc.\nImportance: The financial institution need to collect data, for example, transaction data, account history, customers’ data, etc, to analyze and predict the financial fraud to avoid the financial fraud in the future.\nWhy the reader should continue: there are more and more new fraud methods came up, the past machine learning model might cannot apply to the new fraud methods.\nwhat work had done: The researchers had done many data collection, data preprocessing, and set up machine learning models to do the logistic regression, decision trees and random forest, etc. They also had done so many behavioral analysis to analyze transaction frequency, amount, time of day, etc.\nwhat are the “different points of views”/interpretations in the literature: There are many different points of views related to the financial fraud. From economic Perspective, they may analyze the costs of fraud prevention and the actual costs of fraud. From the data scientists’ perspective, they wish to focus more on using data-driving approaches to prevent fraud. There are also many other perspectives from different researchers.\nwhat exploring: In this area, we need to explore more about the relations between customers’ behavior and the fraud methods\ngoals and hypothesis: clean the data I collect and find some factors that impact the financial fraud and make a fraud detetction model."
  },
  {
    "objectID": "index.html#questions-to-address",
    "href": "index.html#questions-to-address",
    "title": "DSAN-5000: Introduction",
    "section": "2. 10 questions to address",
    "text": "2. 10 questions to address\n\nCan customer reviews and sentiment analysis on social media reflect the public’s reaction to the products?\nHow does Apple’s stock performed?\nWhat are the trends in iPhone stocks sale volume over the last decade?\nWhat kind of electronic products can be seem as a good product to buy?\nHow does the pricing of Apple products compare with competitors over time, and what is the impact on market share and consumer choice?\nHow can we predict whether Apple’s products would be popular in the future?\nCan we use machine learning to identify key factors that predict an Apple product’s popularity based on historical data?\nWhat kind of models we can used to predict people’s attitude with reviews?\nCan decision tree analysis help in identifying whether a product is a successful mobile phone product?\nHow can we eliminate the unnecessary feature for predicting the Apple’s products?"
  },
  {
    "objectID": "index.html#realated-articles",
    "href": "index.html#realated-articles",
    "title": "DSAN-5000: Introduction",
    "section": "3. Realated Articles",
    "text": "3. Realated Articles\n\nA comparative study of online consumer reviews of Apple iPhone across Amazon, Twitter and MouthShut platforms2\nAbstract\n\nThe goal of this study is to create a classification system for online customer reviews by examining the differences in these evaluations across various internet platforms and identifying their distinctive features. It uses a combination of qualitative and quantitative research techniques. The study discovers that reviews of e-commerce websites cover a greater variety of subjects. Specialized review sites, on the other hand, offer reviews that are more in-depth but cover fewer subjects. Social media reviews find a middle ground between these two extremes. The examination reveals recurring motifs and broad meta-themes in these assessments. A system of classification for the reviews is devised on the basis of these insights, linking the reasons for the reviews from every platform to the recognized meta-themes. This study provides business managers with useful information to enhance their online review tactics.\n\n\n\nThe influence of brand popularity, perceived quality, price, and need on purchase intention iPhone products in Purwokerto3\nAbstract\n\nThis study, which is survey-based, aims to investigate how consumer needs, perceived quality, price, and brand popularity affect the decision of Purwokerto City residents to buy iPhone products. Purposive sampling was used to choose a sample of 120 people for the study. Data analysis showed that brand popularity may potentially have a negative effect on purchase intentions, not a major one. Price, on the other hand, has a negative impact on buying intentions. However, the intention to purchase is positively influenced by both perceived quality and consumer necessity. One important finding from this study is that purchase intentions are not always influenced by brand popularity. Rather, the most significant element influencing purchase intentions is shown to be consumer need."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "DSAN-5000: Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKundu, Supratim, and Swapnajit Chakraborti. “A Comparative Study of Online Consumer Reviews of Apple IPhone across Amazon, Twitter and MouthShut Platforms.” Electronic Commerce Research, vol. 22, no. 3, 2022, pp. 925–50, https://doi.org/10.1007/s10660-020-09429-w.↩︎\nKundu, Supratim, and Swapnajit Chakraborti. “A Comparative Study of Online Consumer Reviews of Apple IPhone across Amazon, Twitter and MouthShut Platforms.” Electronic Commerce Research, vol. 22, no. 3, 2022, pp. 925–50, https://doi.org/10.1007/s10660-020-09429-w.↩︎\nRizaldi, Herna. ” The influence of brand popularity, perceived quality, price, and need on purchase intention iPhone products in Purwokerto.” Jurnal Akuntansi, Manajemen dan Ekonomi [Online], 24.2 (2022): 14-22. Web. 7 Nov. 2023↩︎"
  },
  {
    "objectID": "decision_trees/decision_trees.html",
    "href": "decision_trees/decision_trees.html",
    "title": "Introduction",
    "section": "",
    "text": "Decision Trees\n\nIntroduction\nA tree structure that describes instance classification is called a classification decision tree model. The nodes and directed edges create a decision tree. Inner nodes and leaf nodes are the two distinct types of nodes. Leaf nodes indicate a class, while internal nodes represent as a feature or attribute. A decision tree, additionally referred to as a binary or multinomial tree, is a predictive analytical model displayed as a tree structure."
  },
  {
    "objectID": "data_gathering/data_gathering.html",
    "href": "data_gathering/data_gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "For this project, the main point is to find people’s attitudes towards Apple Products and evaluate what kind of mobile products would attract the public to purchase. Thus, I need to see both text data to reflect people’s attitudes toward Apple Products and record data to evaluate the products’ popularity and people’s willingness to buy. Thus, I need to search online to find either people’s comments about Apple Products or the news reports related to Apple products. Also, I need to find some datasets to evaluate what kinds of mobile products would be popular in the market so that when the new generation of Apple Products comes out, we can predict the popularity of the new products.\n\n\n\n\nNews API: Gathering news’ reports to reflect the public attitudes towards Apple products.\nNews Webpage: Collect more news reports to reflect the public attitudes\nWeibo: Crawl comments from the social media to see the users’ reaction to the new Apple Products\nYahoo! Finance: collect Apple’s recent stock performance and this also can reflect the apple products’ popularity.\nMobile phone ratings: trying to find a dataset that have ratings related to all different functions of the mobile phones and what their popularity with the mobile phones.\n\n\n\n\n\nPython\nR\nNews API\nDownloading Dataset\n\n\n\n\n\n\n\nPython, Web crawl\nDataset Collecrion: Weibo is a Chinese social platform that Chinese public sharing thoughts and post articles. There are 52 illion daily active users via that social media.I crawled data from Weibo to catch some hot posts about people’s reaction to the new publishion of Apple products. Most of the articles relected strong sentiments towards the products\nweibo data gathering code (python)\nraw data for weibo(python)\nsample raw data \nDataset Exaplaination: From the view of dataset, this is a text dataset and we can see the users’ ID and the publish dates and also the contents and commens(in Chinese).\n\n\n\n\n\nPython, News API\nDataset Collection: crawl data by using NewsAPI with Python and try to extract some key words from the news. The news API can help me to find the most updated articles related to the topics I am trying to search and based on the contens, I can analyze those text to see what’s the attitude of media towards the iPhone.\nNews data gathering code (python)\nraw data for newsAPI(python)\nsample raw data \nDataset Exaplaination: As shown above, this is a text dataset and we can read all the articles that News API collected.\n\n\n\n\n\nR, Rvest\nDataset Collection: Revest is also an useful tool to help up to crawl conetent from the website we want. I used Rvest to craw text with R and get content from the news website I selected.\nNews webpage data gathering code (R)\nraw data for news webpage(R)\nsample raw data \nDataset Exaplaination: as shown in above, as shown above, this is also a text dataset and we can read all the articles that Rvest collected.\n\n\n\n\n\nDownloading from Yahoo! Finance\nDataset Collection: Stock price and trend also can reflect the company’s popularity and how the public’s attitude towards the company. Thus I downloaded the Apple stock’s recent stock price and trying to analyze its stock trend.\nDataset source: Apple Stock in Yahoo! Finance\nsample raw data \nDataset Explaination: As shown in the dataset, this is a record dataset. We can see there’s the date, Openprice, highest price, lowest Price, close price, and trading volume. Based on this dataset, for nect next step we can analyze its stock prices changes.\n\n\n\n\n\nCollecting from Dxomark\nDataset collection: The dataset that evaluate the functions for a mobile phone can help me to identify the public’s buying willingness to the product. Thus I collected the dataset from the DXOMARK website that reflect the device’s performance and the quality of the user experience.\nDataset source: DXOMARK smartphone reviews\nsample raw data \nDataset Explaination: As shown in the dataset, this is a record dataset. The dataset has its ranking and devices’ name, its Launch price, Launche date, camera rating, selfie rating, audio rating, display rating and battery ratings. All those ratings are important factors that impact the customers’ buying inetent. Thus this dataset will help me to predict wether a smartphone product will be popular based on the ratings.\n\n\n\n\n\nKaggle dataset\nDataset collection: This dataset is both record and text data. I download it from the kaggle website. This dataset can help me better understand people’s reaction and reviews towards apple products.\nDataset source: Kaggles\nsample raw data \nDataset Explaination: the dataset includs the customers’ reviews and ratings from Amazon for iphone 11. The text contained people’s reviews and comments. The record data are related to people’s ratings, helpful counts, review country, etc. For this dataset, I will simply use for the data explortion process for better understand this topic.\n\n\n\n\nIn this part, I collected 6 different datasets that related to my research topic, Apple Products’ Popularity. The datasets are including text data and record data. The methods I used to collect data are using news API, crawling data with Python, Rvest with R, and downloading related data directly from the website.\nNext step I will clean and modified the data as need for preparing for the future model training."
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "In data analysis or data science initiatives, exploratory data analysis, or EDA, is an essential stage. Understanding the data, its structure, and its patterns requires a range of procedures and methods. Here are some general steps for the EDA process:\n\nData collection\nData cleaning\nData visualization\nStatistical analysis\nFeature engineering\nCorrelation analysis\nHypothesis Testing\nDocumenting Insights and Findings\n\n\n\n\nIn this part, I will use visualization tools to help the audience know more about my project, which will include text clouds, statistic distribution, correlation heatmap, etc. This will help the audience to understand the general partterns of the data, understand the analysis of the data.\n\n\n\n\n\n\nIn this text dataset, the type for this dataset is categorical data.\n\n\n\n\n\ncode in the news_cleanning.ipynb\n\n\n\n\nExplaination: After I cleaned the data with Numpy and Pandas package as I needed, I use the Wordcloud package to output the most frequent appeared words in those articles. As shown, most words are describing its functions and devices. Most are natural, but also some obvious positive words, like “best, right”. Thus the public’s opinion are more in natrual and positive.\n\n\n\ntext cloud for news articles\n\n\n\n\n\n\n\n\nExplaination: I cleared out all the stopwords as needed, and get the most frequent words out to see the correlation between those key words. We can see that the color of words more close to white is more positive related, and the color more close to black is negative related.\n\n\n\ncorrelation heatmap for news articles\n\n\n\n\n\n\n\nfrom the analysis from above, we can see most of the key words from the news are positive or natural, which means, the public has postive attitude towards apple products.\n\n\n\n\n\n\n\n\nFor this dataset, it’s all numerical data. The open/close price are closely relate to each other\ncode in the code/EDA/apple_stock.ipynb file\n\n\n\n\n\n\n\n\nstatistics for apple stock\n\n\nAs shwon in the table, we can the standard deviation for the apple stock is around 19, it show a floating with the apple stock within days.\nhowever the max and min are similar for each open/close price, shows a stable status for the apple stock.\n\n\n\n\n\n\n\nExplaination: I collected the dataset of stock price from Yahoo! finance, and used Matplotlib, plotly and seaborn to plot the stock price in line plot with the open price and the date.\n\n\n\nopen price for apple stock\n\n\nWe can see from this plot that the price is shift upward which is in a increasing trend, which indicates that the apple stock are more popular and everyone is wiliing to buy the shares so the prices is keeping increasing.\n\n\n\n\n\nExplaination: I collected the dataset of stock price from Yahoo! finance, and used Matplotlib, plotly and seaborn to plot the stock trading volume in each date in line plot\n\n\n\nopen price for apple stock\n\n\nAs shown, we can see the trading volume are decreasing during the time, which may indicate that the apple’s stock are less people buying, and it may related to the price.\n\n\n\n\n\n\n\n\n\nOutliers for apple stock\n\n\nAs shown, there’s no outliers for this stock, so, every points can be inclued in evaluating this data.\n\n\n\n\n\nFrom the analysis from above, we can see the apple stock price is still increasing, and seems still be popular to the public. This may indicate the Apple product is still popular in the market.\n\n\n\n\n\n\n\n\nFor this dataset, below are their data type.\n\n\n\nMobile Phone data type\n\n\nThe launch price, camera, selfie, audio, display, battery ratings are all related to the rank.\n\n\n\n\n\n\n\n\n\n\nThe relation of Launch Price and Buing Intent\n\n\nUsed Seaborn\nIt shows that the launch price actually determined whether willing to buy. When the price is around 750-1250 dollars, people are very willing to buy.\n\n\n\n\n\n\n\n\nThe relation between Buying Intent and Camera Rating\n\n\nUsed Seaborn\nIt also shows people willingness to buy are connected to the camera’s quality. the camera score that people very willing to buy is around 130-160 range. Also, the not willing’s box range is the widest, range from about 50-110. The median score for the not willing is around 80, and for moderately_willing is around 120, for very_willing is about 135.\n\n\n\n\n\n\n\n\nTop 50 Ranking Brands\n\n\nUsed Matplotlib\nAs shown in the graph, there are 20% Top 50 ranking phones are apples. Which shows, Apple is still one of the most popular brands in the market.\n\n\n\n\n\n\n\n\n\nThe correlation between features\n\n\nUsed heatmap\nIt semmed that camera and launch price are correlated to each other. And the Launch price camera score and audio score are negatively correlated to each other.\n\n\n\n\n\n\n\n\nstatistics for mobile phone\n\n\nthe mean Launch price is around $773.5 and the mean score for selfie is around 130, which is obviously higher than others (this is becuase I used mean to insert missing value)\n\n\n\n\n\nfrom the analysis we can analyze what kind of features would be popular around customers. This can be used for analysis whether apple would be best product among choices. It still prove that Apple is popular among customers.\n\n\n\n\n\n\n\n\nThis is a text record data, and it is a categorical data.\n\n\n\n\n\nfrequency distribution\n\n\n\nIphone 11 ratings\n\n\nUsed Seabron\nFrom the graph, we can see most people around 3600 people give 5.0 ratings and only about 350 people gives 1.0 and 2.0 rating. Which shows that most customers satisfied the apple products\n\n\n\n\n\n\n\n\nstatistics for Ipohone11 ratings\n\n\nThe mean rating is 4.48 which is very high, shows that everyone like iphone 11 and gave a high rating. The mean rating being at the upper limit of the scale is consistent with the value of 1.09, which shows some variation in the ratings but not an unduly high level of variability.\n\n\n\n\n\nFrom the analysis we can analyze that most customers satisfied apple’s products, so apple may be still popular in the market."
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Methods & Codes",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf=pd.read_csv(\"../../../data/01-modified-data/after_clean_mobile_phone_rating.csv\")"
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html",
    "href": "dimensionality_reduction/dimensionality_reduction.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Dimensionality Reduction"
  },
  {
    "objectID": "conclusion/conclusion.html",
    "href": "conclusion/conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "The goal for this project is identify whether Apple products are still popular in the market, and find out public’s reaction to Apples’ new products. Also, I want to find out what kind of of quality of a mobile phone should have that will attract the public to buy. Thus, I should train some models that can predict a product’s populariy based on its functions, so when iPhone’s new generation came out, I can use thid model to predict its popularity.\n\n\n\n\n\nIn this part, I collected 3 text datasets and 2 record datasets by using API, Web Crawling, downloading methods by using Python and R. Most text datasets are related to the public’s reviews and comments towards Apple products,and one of the record datasets are related to Apple stock, and another is related to the mobile phones’ function rating. All the datasets can help me to dig into this research topic.\nHere’s some sample raw datasets:\nraw text data \nraw record data \n\n\n\nIn this part, I did data cleanning process for the datasets that I needed for preparing for future data analysis and model training. For the record data, I did fill null values, remove duplicates, handling outliers, convert data types, etc cleaning process. For the text data, I did remove stop words, remove special characters and punctuation, tokenization, stemming and lemmatization, remove white spaces, etc cleaning process.\nHere’s some sample cleaned datasets:\ncleaned text data \ncleaned record data \n\n\n\nIn this part, I did some exploratory data analysis for the cleaned datasets for better knowing the datasets. I used visualization tools to help the audience know more about my project, which will include text clouds, statistic distribution, correlation heatmap, etc.\nHere are some examples: \nHere’s some key words that appeared most frequently in the news dataset that I collected. As we can see, most words are natural and positive towards apple products.\n\n\n\nopen price for apple stock\n\n\nFrom the Apple stock’s plot we can see from the past year, the stock price is in the increasing trend, which means more poeple are willing to buy Apple’s stock, which means Apple is still popular in the market.\n\n\n\nTop 50 Ranking Brands\n\n\nAs shown in the pie chart, in the Top 50 ranking phones, 20% are apples. Which means there are 10 iphones are ranked in top50. This also shows Apple is still one of the most popular brands in the market.\n\n\n\nIn this part, I used K-means, DBSAN, Hierarchical clustering methods to clustered the mobile phone ratings dataset. Based on the hyperparameter turning, here’s the optimal clusters results: K-means: 2 DBSCAN : 4 Agglomerative Hierarchy: 2 Meanshift : 4 Birch : 3\nBased on the analysis process, I think Agglomerative Hierarchy would the best method for this dataset, and the optimal clustering would be 2.\n\n\n\nAgglomerative Hierarchy Clustering\n\n\n\n\n\nIn this part, I used PCA and t-SNE method to reduce the dimension for better analyzing the mobile phone ratings dataset. Based on the visualization result, I believe PCA would be a better method for this dataset.\n\n\n\nPCA\n\n\nAs shown in the graph, the red line intersect the blue line at number of comonents around = 4. So, when the number of compnents is larger than 4 which is 5, we can make 95% variance expliained. There for we should reduce the number of components to 5.\n\n\n\n\n\n\nIn this part, I used decision tree and random forest to do the classification. I training the model based on the mobile phone prices, the function ratings of the mobile phones to predict the public’s the buying intent to the products. - Decision Tree The optimal levels are 3\n\n\n\nDecision tree\n\n\nAnd the final test showed the model performed well: ACCURACY: 0.967741935483871 NEGATIVE RECALL (Y=0): 1.0 NEGATIVE PRECISION (Y=0): 0.9375 POSITIVE RECALL (Y=1): 0.9375 POSITIVE PRECISION (Y=1): 1.0 \n\nRandom Forest The result for the random forst is similar to the decision tree, and I believe the decision tree is better predict the model. More content related to random forest can be found in the Decision Tree Tab.\n\n\n\n\nIn this part, I used Naive Bayes algorithmn predict the lebels for the mobile phone ratings dataset and also the news text dataset. I used feature selection of variance thredhold method with hyperparameter tuning for better training the model.\nFinal result showed the naive bayes predicted my record dataset well. \n\n\n\nConfusion Metrics\n\n\nFinal result also showed the naive bayes predicted my text dataset well.\n\n\n\nEvaluation Metrics\n\n\n\n\n\nConfusion Metrics\n\n\nMore analysis can find in the Naive Bayes tab.\n\n\n\n\nBased on the research above, I belive I can explain all the questions listed in the introduction well:\n\nCan customer reviews and sentiment analysis on social media reflect the public’s reaction to the products?\nYes, based on the sentiment analysis and key words extraction, we can see the public’s reaction easily.\nHow does Apple’s stock performed? Apple stock performed well in the past year.\nWhat are the trends in iPhone stocks sale volume over the last decade? the sale volumn are not increasing that much which may be a good signal for the current shareholder.\nWhat kind of electronic products can be seem as a good product to buy? higher in price, higher in the selfie, audio, etc functions ratings.\nHow does the pricing of Apple products compare with competitors over time, and what is the impact on market share and consumer choice? Apple’s product are kind of higher than the other competitors’ products\nHow can we predict whether Apple’s products would be popular in the future? From its function ratings, we can predict whether the public willing to buy or not.\nCan we use machine learning to identify key factors that predict an Apple product’s popularity based on historical data? Yes, based on the correlation map, we can see the price and selfie function are highly corelated and impacted the products’ popularity\nWhat kind of models we can used to predict people’s attitude with reviews? we can use the naive bayes or decision tree model to predict\nCan decision tree analysis help in identifying whether a product is a successful mobile phone product? Yes, we can use decision tree actually is the best method to predict the label.\nHow can we eliminate the unnecessary feature for predicting the Apple’s products? We can use some feature selection methods, like variance thredhold to eliminate some features.\n\n\n\n\nFrom this project, we can know more about the public’s review and reaction on apple products and how to predict a new generation iphone’s popularity. From all the research above, I can conclude that Apple products are still popular in the market, and people’s reviews are more in postive way. The Apple stock price also reflect the popularity of Apple brand. Also the ratings of apple products are still very high compare to other brands.\nThus, apple products are still very popular and worthy to buy based on its functionality ratings. We can also use those ratings based on machine learning models to predict the popularity of future products."
  },
  {
    "objectID": "eda/eda.html#introduction-to-eda",
    "href": "eda/eda.html#introduction-to-eda",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "In data analysis or data science initiatives, exploratory data analysis, or EDA, is an essential stage. Understanding the data, its structure, and its patterns requires a range of procedures and methods. Here are some general steps for the EDA process:\n\nData collection\nData cleaning\nData visualization\nStatistical analysis\nFeature engineering\nCorrelation analysis\nHypothesis Testing\nDocumenting Insights and Findings"
  },
  {
    "objectID": "eda/eda.html#news-articles-dataset",
    "href": "eda/eda.html#news-articles-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "In this text dataset, the type for this dataset is categorical data.\n\n\n\n\n\ncode in the news_cleanning.ipynb\n\n\n\n\nExplaination: After I cleaned the data with Numpy and Pandas package as I needed, I use the Wordcloud package to output the most frequent appeared words in those articles. As shown, most words are describing its functions and devices. Most are natural, but also some obvious positive words, like “best, right”. Thus the public’s opinion are more in natrual and positive.\n\n\n\ntext cloud for news articles\n\n\n\n\n\n\n\n\nExplaination: I cleared out all the stopwords as needed, and get the most frequent words out to see the correlation between those key words. We can see that the color of words more close to white is more positive related, and the color more close to black is negative related.\n\n\n\ncorrelation heatmap for news articles\n\n\n\n\n\n\n\nfrom the analysis from above, we can see most of the key words from the news are positive or natural, which means, the public has postive attitude towards apple products."
  },
  {
    "objectID": "eda/eda.html#apple-stock-dataset",
    "href": "eda/eda.html#apple-stock-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "For this dataset, it’s all numerical data. The open/close price are closely relate to each other\ncode in the code/EDA/apple_stock.ipynb file\n\n\n\n\n\n\n\n\nstatistics for apple stock\n\n\nAs shwon in the table, we can the standard deviation for the apple stock is around 19, it show a floating with the apple stock within days.\nhowever the max and min are similar for each open/close price, shows a stable status for the apple stock.\n\n\n\n\n\n\n\nExplaination: I collected the dataset of stock price from Yahoo! finance, and used Matplotlib, plotly and seaborn to plot the stock price in line plot with the open price and the date.\n\n\n\nopen price for apple stock\n\n\nWe can see from this plot that the price is shift upward which is in a increasing trend, which indicates that the apple stock are more popular and everyone is wiliing to buy the shares so the prices is keeping increasing.\n\n\n\n\n\nExplaination: I collected the dataset of stock price from Yahoo! finance, and used Matplotlib, plotly and seaborn to plot the stock trading volume in each date in line plot\n\n\n\nopen price for apple stock\n\n\nAs shown, we can see the trading volume are decreasing during the time, which may indicate that the apple’s stock are less people buying, and it may related to the price.\n\n\n\n\n\n\n\n\n\nOutliers for apple stock\n\n\nAs shown, there’s no outliers for this stock, so, every points can be inclued in evaluating this data.\n\n\n\n\n\nFrom the analysis from above, we can see the apple stock price is still increasing, and seems still be popular to the public. This may indicate the Apple product is still popular in the market."
  },
  {
    "objectID": "eda/eda.html#iphone_11_review_dataset",
    "href": "eda/eda.html#iphone_11_review_dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "This is a text record data, and it is a categorical data.\n\n\n\n\n\nfrequency distribution\n\n\n\nIphone 11 ratings\n\n\nUsed Seabron\nFrom the graph, we can see most people around 3600 people give 5.0 ratings and only about 350 people gives 1.0 and 2.0 rating. Which shows that most customers satisfied the apple products\n\n\n\n\n\n\n\n\nstatistics for Ipohone11 ratings\n\n\nThe mean rating is 4.48 which is very high, shows that everyone like iphone 11 and gave a high rating. The mean rating being at the upper limit of the scale is consistent with the value of 1.09, which shows some variation in the ratings but not an unduly high level of variability.\n\n\n\n\n\nFrom the analysis we can analyze that most customers satisfied apple’s products, so apple may be still popular in the market."
  },
  {
    "objectID": "eda/eda.html#mobile_phone_rating-dataset",
    "href": "eda/eda.html#mobile_phone_rating-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "For this dataset, below are their data type.\n\n\n\nMobile Phone data type\n\n\nThe launch price, camera, selfie, audio, display, battery ratings are all related to the rank.\n\n\n\n\n\n\n\n\n\n\nThe relation of Launch Price and Buing Intent\n\n\nUsed Seaborn\nIt shows that the launch price actually determined whether willing to buy. When the price is around 750-1250 dollars, people are very willing to buy.\n\n\n\n\n\n\n\n\nThe relation between Buying Intent and Camera Rating\n\n\nUsed Seaborn\nIt also shows people willingness to buy are connected to the camera’s quality. the camera score that people very willing to buy is around 130-160 range. Also, the not willing’s box range is the widest, range from about 50-110. The median score for the not willing is around 80, and for moderately_willing is around 120, for very_willing is about 135.\n\n\n\n\n\n\n\n\nTop 50 Ranking Brands\n\n\nUsed Matplotlib\nAs shown in the graph, there are 20% Top 50 ranking phones are apples. Which shows, Apple is still one of the most popular brands in the market.\n\n\n\n\n\n\n\n\n\nThe correlation between features\n\n\nUsed heatmap\nIt semmed that camera and launch price are correlated to each other. And the Launch price camera score and audio score are negatively correlated to each other.\n\n\n\n\n\n\n\n\nstatistics for mobile phone\n\n\nthe mean Launch price is around $773.5 and the mean score for selfie is around 130, which is obviously higher than others (this is becuase I used mean to insert missing value)\n\n\n\n\n\nfrom the analysis we can analyze what kind of features would be popular around customers. This can be used for analysis whether apple would be best product among choices. It still prove that Apple is popular among customers."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html",
    "href": "Naive_Bayes/Naive_Bayes.html",
    "title": "Introduction to Naive Bayes:",
    "section": "",
    "text": "The process of training the Naive Bayes algorithm is actually the process of solving each a priori probability P(Ai) and conditional probability P(Bj|Ai), and then in the prediction and classification process, it is only necessary to substitute each probability value obtained during the training process into the Bayesian formula, so as to obtain the relative probability of the results of the classification labels under the current features. The one with higher relative probability is naturally used as the predicted label.\n\n\n\n\n\n\\(P(Y|X) = (P(X|Y) * P(Y))/P(X)\\)\nP(A|B) is called the a posteriori probability, for the target to be found\nP(A) is the a priori probability, which can be statistically derived from a large amount of data, or empirically provided when the amount of data is small.\nP(B|A) is the conditional probability, which can be statistically derived from a large amount of data, and is usually given by great likelihood estimation, and is actually also the a priori probability\n\n\n\n\n\nGaussian Naive Bayes: This variant of Naive Bayes is best suited for datasets where the features are continuous and follow a Gaussian or normal distribution. It assigns a Gaussian distribution to each class and predicts the class of a new data point based on these distributions. Common applications include problems that involve continuous variables, as is often found in real-world datasets.\nMultinomial Naive Bayes: This type excels with discrete data and finds frequent application in text categorization tasks, where text is converted into word frequency vectors. However, it can also work with tf-idf vectors. It computes the conditional probability of a word given a class, making it a good choice for problems involving discrete data.\nBernoulli Naive Bayes: This method is ideal for binary or boolean features and is typically used in text classification tasks where the presence or absence of a word is more important than its frequency. It assumes that all our features are binary-valued and models the input data with a multivariate Bernoulli distribution.\n\n\n\n\nI will use Naive Bayes Algorithmn to train 2 datasets, the objective for using this is for accurately predicting the lebels for my datasets:\n\nMobile phone rating: This is a record dataset. There are features like mobiles camera scores, audio scores, display scores, etc, and those related to the customers’ willingness to buy this phone or not. The main goal for training this dataset is for repeict customer’s willingness to buy a phone or not depends on those feature scores.\nIphone 11 reviews: This is a text dataset. The text are customers reviews. The main goal for this data training is by through the reviews, predict the attitutes of the customers."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#introduction-to-naive-bayes",
    "href": "Naive_Bayes/Naive_Bayes.html#introduction-to-naive-bayes",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The process of training the Naive Bayes algorithm is actually the process of solving each a priori probability P(Ai) and conditional probability P(Bj|Ai), and then in the prediction and classification process, it is only necessary to substitute each probability value obtained during the training process into the Bayesian formula, so as to obtain the relative probability of the results of the classification labels under the current features. The one with higher relative probability is naturally used as the predicted label.\n\n\n\n\n\n\\(P(Y|X) = (P(X|Y) * P(Y))/P(X)\\)\nP(A|B) is called the a posteriori probability, for the target to be found\nP(A) is the a priori probability, which can be statistically derived from a large amount of data, or empirically provided when the amount of data is small.\nP(B|A) is the conditional probability, which can be statistically derived from a large amount of data, and is usually given by great likelihood estimation, and is actually also the a priori probability"
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#prepare-dataset",
    "href": "Naive_Bayes/Naive_Bayes.html#prepare-dataset",
    "title": "Introduction to Naive Bayes:",
    "section": "Prepare Dataset",
    "text": "Prepare Dataset\n\ncreate labels\nI create labels for the mobile phone by its rankings\n(code in codes/data-cleaning/mobile_phone_rating.ipynb).\n\nThe top 50 ranking phones is Very Willing\nThe 51-100 ranking phones is Moderately Willing\nThe 101-last ranking ophones is Not Willing\n\n\n\nSplit data\n(code in codes/naive_bayes/mobile_phone_rating_training)\n\nI split data into traning set 70%, validation set 15%, and test set 15%.\n70% in training will increase the accuracy for the model learning. And the rest each 15% for better identify the training results."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#feature-selection",
    "href": "Naive_Bayes/Naive_Bayes.html#feature-selection",
    "title": "Introduction to Naive Bayes:",
    "section": "Feature Selection",
    "text": "Feature Selection\n(code in codes/naive_bayes/mobile_phone_rating_training)\nAccording to the number of features, I found out it nearly has no impact on the trainig. - \nThus I chose to use Vairance Thredhold for this data, after the selection, I find out the optimal thredhold. It turns out the optimal thredhold is 26.51, and the accuracy is around 88%."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#naive-bayes-with-labeled-record-data",
    "href": "Naive_Bayes/Naive_Bayes.html#naive-bayes-with-labeled-record-data",
    "title": "Introduction to Naive Bayes:",
    "section": "Naive Bayes with Labeled Record Data",
    "text": "Naive Bayes with Labeled Record Data\n(code in codes/naive_bayes/mobile_phone_rating_training)\n\nHowever, after I tried several times for selecting the vairance thredhold, becuase the feature selection process above elimiating some cornor cases, but there are not that many features in my model, if i eliminating them, it would decrease my model accuracy. So after all, I found the optimal thredhold should be 45, so I choose to use that for my model.\nIn that way, the ‘Selfie’ column will be deducted, which make sense because when I did data cleaning process, there are many nulls and I use the mean to fill null.\n\n\nfinal result\n\nEvaluation Matrices\n\n\n\n\nEvaluation Metrics\n\n\nAs shown above, the accuracy of my model is 91.3% and which is very high, shows it’s a good model.\nPrecision is comparely low for the “not will”, but it’s ok, since the number of that labels are not too many.\nF-1 score and recall performs well.\nMacro Avg: Precision: 86%, Recall: 94%, F1-Score: 89% are all very high schore, shows it’s a good model to fit in.\nWeighted Avg: Precision: 93%, Recall: 91% , F1-Score: 92%, shows it’s a good model to fit in.\nIt did not show any overfitting or underfitting,since the test scores perform well.\n\n\n\nconfusion matrices\n\n\n\n\nConfusion Metrics\n\n\nAs shown above, the accuracy for predicting labels are very high. Only 2 labels are inaccurately predicted."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#conclusion",
    "href": "Naive_Bayes/Naive_Bayes.html#conclusion",
    "title": "Introduction to Naive Bayes:",
    "section": "Conclusion",
    "text": "Conclusion\nThe model predict my dataset well. After he variance Thredhold feature selection, the accuracy increased from about 88% to 91.3%. Also, the model did not show overfitting/underfitting because of its traning accuracy and test accuracy are all around 90%. Thus this model can well predict how those scores impact people’s buying willingness."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#prepare-dataset-1",
    "href": "Naive_Bayes/Naive_Bayes.html#prepare-dataset-1",
    "title": "Introduction to Naive Bayes:",
    "section": "Prepare Dataset",
    "text": "Prepare Dataset\n\ncreate labels\nI create labels for the mobile phone by its ratings\n(code in codes/naive_bayes/iphone_11_review_training.ipynb).\n\nThe top 50 ranking phones is Very Willing\nThe 51-100 ranking phones is Moderately Willing\nThe 101-last ranking ophones is Not Willing\n\n\n\nSplit data\n(code in codes/naive_bayes/iphone_11_review_training.ipynb).\n\nI split data into traning set 80%, and test set 20%.\n80% in training will increase the accuracy for the text model learning. Also, 20% of test dataset is enough for giving good results."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#feature-selection-1",
    "href": "Naive_Bayes/Naive_Bayes.html#feature-selection-1",
    "title": "Introduction to Naive Bayes:",
    "section": "Feature Selection",
    "text": "Feature Selection\n(code in codes/naive_bayes/iphone_11_review_training.ipynb)\n\n\n\n\nfeature selection\n\n\nAs shown in the graph, when the number of features close to 600-700, the test and training accuracy score did not improve. And after that, the model indicate it’s overfitting because the taining accuracy is high but test accuracy is not that hight. So our opimal feature selection should around 600-700.\n\n\n\nrunning time\n\n\nAs shown in the graph, when the number of features close to 1000 the running time increased a lot\n\nBased on those observation, and also from the accuracy scores of each number of features, I get the optimal number of features are 640. With this feature number, the accuracy and running time are optimal."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#naive-bayes-with-labeled-text-data",
    "href": "Naive_Bayes/Naive_Bayes.html#naive-bayes-with-labeled-text-data",
    "title": "Introduction to Naive Bayes:",
    "section": "Naive Bayes with Labeled Text Data",
    "text": "Naive Bayes with Labeled Text Data\n(code in codes/naive_bayes/iphone_11_review_training.ipynb)\n\n\n\n\nEvaluation Metrics\n\n\nAs shown above, the accuracy of my model is 90.11% and which is very high, shows it’s a good model.\nMacro Avg: Precision: 65%, Recall: 58%, F1-Score: 61%, those scores are comparedly not perform very well, it is because some low performance for the natural(0) class, this is also make sense, because there are not many data related to that class.\nWeighted Avg: Precision: 89%, Recall: 90% , F1-Score: 89%, shows it’s a good model to fit.\n\n\nConfusion Matrices\n\n\n\n\nConfusion Metrics\n\n\nAs shown above, the accuracy for predicting right labels are very high.\nIn total, 903 data labels were predicted accurately. The model predited well in the positive and negative classes, but did not perform well in the natural class. This is because there are not so many data of natural."
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#conclusion-1",
    "href": "Naive_Bayes/Naive_Bayes.html#conclusion-1",
    "title": "Introduction to Naive Bayes:",
    "section": "Conclusion",
    "text": "Conclusion\nThe model predicts my dataset well. After the number of feature selection methods, the accuracy can reach 90.11%. However, because of the insufficient data related to the natural label class, the prediction for this class did not perform as well as expected.\nAlso, the model did not show overfitting/underfitting because its training accuracy and test accuracy are all around 90%.\nThus, overall, the model performed well, and it can, based on the text reviews, predict people’s attitudes towards iPhone 11 products."
  },
  {
    "objectID": "index.html#topics-introduction-iphones-popularity-in-the-markey",
    "href": "index.html#topics-introduction-iphones-popularity-in-the-markey",
    "title": "DSAN-5000: Introduction",
    "section": "1. Topics Introduction: iPhone’s popularity in the markey",
    "text": "1. Topics Introduction: iPhone’s popularity in the markey\n\nsummary\nOver the decades, Apple’s products are always the most popular products in the markets. However, with more and more new products introduced to the market. Is Apple products, especially iphone, are still popular in the market?\nImportance\nThe iPhone has significantly influenced technology and culture since its introduction in 2007. It has influenced and impacted a genration, thus everyone curious about how apple performed in the recent years. Its role in shaping consumer behaviors, communication norms, and mobile technology makes it a rich subject for study.\nWhy the reader should continue\nThe choices and preferences of iPhone users can reflect broader trends in consumer behavior. Consumers can pick better products through this research topic and see how consumers react to Apple’s products.\nwhat work had done\nThe researchers had done many data collection, data preprocessing, and set up machine learning models to do the logistic regression, decision trees and random forest, etc. They also had done so many behavioral analysis to analyze the popularity trend for electronic products.\nwhat are the “different points of views”/interpretations in the literature\nSome literature emphasizes Apple’s commitment to innovation and high-quality products as a key factor in its popularity. This view holds that the company’s focus on design, user experience, and robust performance drives consumer preference. Some authors also discuss Apple’s cultural impact, suggesting that its products have become more than just technology but a part of modern culture.\nwhat exploring:\n\nWe can explore how people evaluate an electronic products as a good products.\nWe can explore the stock market for the Apple Products and how it stock prices changes\nWe can collect customers’ review/news on Apple products, to see how everyone like apple’s products.\n\ngoals and hypothesis\nclean the data I collect and find some factors that impact the financial fraud and make a fraud detetction model."
  },
  {
    "objectID": "clustering/clustering1.html",
    "href": "clustering/clustering1.html",
    "title": "Clustering",
    "section": "",
    "text": "In my Mobile Phone Rating Dataset, the features include ratings about mobile phone functions. There are six features: Launch Price, camera rating, selfie rating, audio rating, display rating, and battery rating.\nThe purpose of this clustering analysis is to do unsupervised learning to find out the possible clusters of the dataset and the optimal cluster numbers."
  },
  {
    "objectID": "clustering/clustering.html#k-means",
    "href": "clustering/clustering.html#k-means",
    "title": "Methods & Codes",
    "section": "K means",
    "text": "K means\n\n# for k means clustering we will use the elbow method to find the optimal number of clusters. we will use the inertia_ attribute to find the sum of squared distances of samples to their closest cluster center. we will use the range of 1 to 10 clusters. plot the inertia_ values for each number of clusters. make sure to save it in a dataframe and plot it using matplotlib.\ninertia = []\ndistortions = []\nfor num_clusters in range(1, 11):\n    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n    kmeans.fit(X_normalized)\n    inertia.append(kmeans.inertia_)\n    distortions.append(sum(np.min(cdist(X_normalized,kmeans.cluster_centers_, 'euclidean'),axis=1)) /X_normalized.shape[0])\n\ndf_inertia = pd.DataFrame({'Number of Clusters': range(1, 11), 'Inertia': inertia})\ndf_distortions=pd.DataFrame({'Number of Clusters': range(1, 11), 'distortions': distortions})\n\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/Users/sherryqin/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n# plot distortion and inertia for kmeans, you can either plot them seperately or use fig, ax = plt.subplots(1, 2) to plot them in the same figure. Suggest the optimal number of clusters based on the plot.\nplt.plot(df_inertia['Number of Clusters'], df_inertia['Inertia'], marker='o', linestyle='-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method for Optimal Number of Clusters')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nplt.plot(df_inertia['Number of Clusters'], df_distortions['distortions'], marker='o', linestyle='-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Distortions')\nplt.title('Elbow Method for Optimal Number of Clusters')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nAnalysis\nBy using K means algorithm, we can see the inertia and distortion with different number of clusters by using hyper-parameter elbow methods. From the graphs above, it seemed that 2 is the optimal number of clusters for me dataset. However, it’s not a obvious elbow can be shown from the graph, so maybe the elbow method is not the best method for this dataset."
  },
  {
    "objectID": "clustering/Results.html",
    "href": "clustering/Results.html",
    "title": "Results Analysis",
    "section": "",
    "text": "Optimal Clusters\n\n\nK-means: 2\nDBSCAN : 4\nAgglomerative Hierarchy: 2\nMeanshift : 4\nBirch : 3\n\n\nAnalysis I think the above methods that predict the clusters well are Agglomerative Hierarchy &gt; Birch &gt; DBSCAN &gt; K-means &gt; Meanshift The best method would be Agglomerative Hierarchy for the following reasons.\n\n\nIt’s easier for me to use because I do not need to identify the k clusters before using it. It automatically gives me the optimal result.\nSince my dataset is not that big, using agglomerative is more effective.\nIt can also make me quickly see the hierarchy relationships with my dataset.\nThe final result of the optimal clusters is two, which makes sense because in my original dataset, I labeled them into 3 clusters, and it turned out that for the “nature” cluster, there are not that many datasets belonging to this cluster.\nCompared to K-means, as shown in the graphs of K-means, the elbow could be clearer for me. Compared to DBSCAN and Meanshift’s result, 4 clusters would make the clusters contain insufficient data points.\nAfter observations, I believe the higher of the price and the other function rating features would be separated into a cluster. And the lower of the price and the other function rating features would be separated into another cluster."
  },
  {
    "objectID": "clustering/Results.html#different-methods-comparing",
    "href": "clustering/Results.html#different-methods-comparing",
    "title": "Results Analysis",
    "section": "",
    "text": "Optimal Clusters\n\n\nK-means: 2\nDBSCAN : 4\nAgglomerative Hierarchy: 2\nMeanshift : 4\nBirch : 3\n\n\nAnalysis I think the above methods that predict the clusters well are Agglomerative Hierarchy &gt; Birch &gt; DBSCAN &gt; K-means &gt; Meanshift The best method would be Agglomerative Hierarchy for the following reasons.\n\n\nIt’s easier for me to use because I do not need to identify the k clusters before using it. It automatically gives me the optimal result.\nSince my dataset is not that big, using agglomerative is more effective.\nIt can also make me quickly see the hierarchy relationships with my dataset.\nThe final result of the optimal clusters is two, which makes sense because in my original dataset, I labeled them into 3 clusters, and it turned out that for the “nature” cluster, there are not that many datasets belonging to this cluster.\nCompared to K-means, as shown in the graphs of K-means, the elbow could be clearer for me. Compared to DBSCAN and Meanshift’s result, 4 clusters would make the clusters contain insufficient data points.\nAfter observations, I believe the higher of the price and the other function rating features would be separated into a cluster. And the lower of the price and the other function rating features would be separated into another cluster."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html",
    "href": "dimensionality_reduction/reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "In this tab, I will try to use PCA and t-SNE method to reduce the dimension for better analyzing the mobile phone dataset."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#project-proposal",
    "href": "dimensionality_reduction/reduction.html#project-proposal",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "In this tab, I will try to use PCA and t-SNE method to reduce the dimension for better analyzing the mobile phone dataset."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#code",
    "href": "dimensionality_reduction/reduction.html#code",
    "title": "Dimensionality Reduction",
    "section": "Code",
    "text": "Code\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\ndf=pd.read_csv(\"../../../data/01-modified-data/after_clean_mobile_phone_rating.csv\")\n\n\nx=df[['Launch Price','CAMERA','SELFIE','AUDIO','DISPLAY','BATTERY']]\ny=df[['buying_intent']]\nscaler = StandardScaler()\nX = scaler.fit_transform(x)\n\n\nDimensionality Reduction with PCA\n\n# EIGEN VALUES/VECTOR\nfrom numpy import linalg as LA\n# w, v1 = LA.eig(cov)\nw, v1 = LA.eig(np.cov(X.T))\nprint(\"\\nCOV EIGENVALUES:\",w)\nprint(\"COV EIGENVECTORS (across rows):\")\nprint(v1.T)\n\n\nCOV EIGENVALUES: [2.46422101 1.21865397 0.24126397 0.958379   0.63183561 0.52512012]\nCOV EIGENVECTORS (across rows):\n[[ 0.50024701  0.53945186  0.20733577  0.42438368  0.44295301 -0.19861426]\n [ 0.39766903  0.19014289 -0.67810966 -0.25218719 -0.31386294 -0.42867702]\n [-0.69792826  0.69118266 -0.15187461 -0.05535362  0.03015126 -0.09013136]\n [ 0.21875576  0.21730996 -0.34121012 -0.27020298  0.26679962  0.8026869 ]\n [-0.19511955 -0.26042618 -0.57511145  0.73310744  0.13997149  0.07946605]\n [ 0.13631836  0.2829461   0.16412113  0.3778623  -0.78332594  0.34357475]]\n\n\n\n# PCA CALCULATION\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=6)\npca.fit(X)\nprint('\\nPCA')\nprint(pca.components_)\n\n\nPCA\n[[-0.50024701 -0.53945186 -0.20733577 -0.42438368 -0.44295301  0.19861426]\n [ 0.39766903  0.19014289 -0.67810966 -0.25218719 -0.31386294 -0.42867702]\n [ 0.21875576  0.21730996 -0.34121012 -0.27020298  0.26679962  0.8026869 ]\n [-0.19511955 -0.26042618 -0.57511145  0.73310744  0.13997149  0.07946605]\n [ 0.13631836  0.2829461   0.16412113  0.3778623  -0.78332594  0.34357475]\n [ 0.69792826 -0.69118266  0.15187461  0.05535362 -0.03015126  0.09013136]]\n\n\n\n# # PLOT\nv2=pca.components_\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X[:,0],X[:,1],X[:,2],marker=\".\", cmap=\"viridis\")\nv1=v1*1000\nv2=v2*1000\n\nax.quiver(0,0,0,v1[0,0],v1[1,0],v1[2,0])\nax.quiver(0,0,0,v1[0,1],v1[1,1],v1[2,1])\nax.quiver(0,0,0,v1[0,2],v1[1,2],v1[2,2])\n\nax.quiver(0,0,0,v2[0,0],v2[1,0],v2[2,0])\nax.quiver(0,0,0,v2[0,1],v2[1,1],v2[2,1])\nax.quiver(0,0,0,v2[0,2],v2[1,2],v2[2,2])\nplt.show()\n\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_92317/2939638508.py:5: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  ax.scatter(X[:,0],X[:,1],X[:,2],marker=\".\", cmap=\"viridis\")\n\n\n\n\n\n\nplt.figure(figsize=(8,4))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='95% explained variance')\ncum_explained=np.cumsum(pca.explained_variance_ratio_)\noptimal_components = np.where(cum_explained &gt;= 0.95)[0][0]+1\nplt.axvline(x=optimal_components, color='red', linestyle='--', linewidth=2, label=f'Optimal components: {optimal_components}')\nplt.show()\n\n\n\n\n\n\nObersvation\nAs shown, the cure is an upward shift along with the increase in the number of components. The first component explains more than 60% of the variance and the second component adds approximately 10% more to the explained variance, bringing the total to around 80%. However, the added contribution decreased when the number of components increased.\nWe want to determine the number of components that make 95% cumulative explained variance. Thus, I made a red line when cumulative explained variance=0.95, and as shown in the graph, the red line intersects the blue line at the number of components around = 4. So, when the number of components is larger than 4, we can make variance 95 % explained.\nThis means the number of components that can make 95% cumulative explained variance is 5.\n\n\nDimensionality Reduction with t-SNE\n\nfrom sklearn.manifold import TSNE\n\n\n# 2D\ntsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1000)\nX_tsne = tsne.fit_transform(X)\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\nplt.show()\n\n\n\n\n\n# 3D\ntsne = TSNE(n_components=3, perplexity=30, learning_rate=200, n_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2])\n\nax.set_xlabel('t-SNE Feature 1')\nax.set_ylabel('t-SNE Feature 2')\nax.set_zlabel('t-SNE Feature 3')\nplt.title('3D t-SNE Visualization')\nplt.show()\n\n\n\n\n\n#parameter tuning for t-SNE (perplexity)\nperplexities = [5, 25, 50]\nlearning_rates = [10, 200, 500]\n\nfig, axs = plt.subplots(len(perplexities), len(learning_rates), figsize=(15, 10))\n\nfor i, perplexity in enumerate(perplexities):\n    for j, learning_rate in enumerate(learning_rates):\n        tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=learning_rate, n_iter=1000)\n        X_tsne = tsne.fit_transform(X)\n        axs[i, j].scatter(X_tsne[:, 0], X_tsne[:, 1])\n        axs[i, j].set_title(f'Perplexity: {perplexity}, Learning Rate: {learning_rate}')\n\nplt.show()\n\n\n\n\n\n\nObservation\nPerplexity: This t-SNE parameter influences how well the local and global components of your data are balanced. Learning Rate: This variable also affects how well the algorithm matches the data. It can be represented as the size of the step that the algorithm uses to discover the representation. As shown in the graph: * Perplexity = 5 with learning rate increasing, the points become denser, which indicating a better balance between local and global perspective. * Perplexity = 25 with learning rate increasing, the points become denser. When the learning rate=200, the structure is more obvious. However when learning rate reach 500, the points are too dense, which may refelct the learning rate may be too high. * Perplexity = 50 with learning rate increasing, the points become denser as well, but not show the structure clearly, so it may not a good fit.\nFrom the observation above, I believe the perplexity=25-50 and learning rate=20 might be the best balance for visualizing the data structure.\nHowever, Compared to PCA, I think t-SNE is less practical to this dataset than PCA because the structure of pattern of t-SNE for this dataset is not so easy to recognize."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#project-report---evaluation-and-comparasion",
    "href": "dimensionality_reduction/reduction.html#project-report---evaluation-and-comparasion",
    "title": "Dimensionality Reduction",
    "section": "Project Report - Evaluation and comparasion",
    "text": "Project Report - Evaluation and comparasion\n\nPCA\nStrength:\n\nPCA can be both used in the small, simple constructed dataset and large complicated dataset.\nIt reduce the noise in the dataset\npreserve the global structure\nworks well with linear dimensionality reduction\n\nWeakness:\n\ndoes not involve hyperparameter tuning\nOversimplification of the data\nloss of valuable information\n\n\n\nt-SNE\nStrength:\n\ninvolve hyperparameters tuning\nPreserves Local and Global Structure\n\nWeakness:\n\nWhen we are trying to reducing the dimension, the distance between points in low dimension are actually not match with the actual distance of the high dimension\nIn the high dimension, if the distance between points are large, but when we convert to low dimension, the distance may be smaller.\nIt is computationally intensive and may not scale well to extremely large datasets 3\n\n\n\nConclusion\nIf the dataset is more linear and the dataset size is small, using PCA would be better.\nIf the dataset is not linear and the dataset size is large, using t-SNE would be better.\nI think PCA is better to use in this dataset because from the plot of t-SNE, it’s really hard to recognize the pattern, but from the result of PCA, we can easily find out how many components we want to keep. So, this dataset may be more in a linear relationship. Also, my dataset is not large, so it’s better to use PCA over t-SNE.\nIn conclusion, when we are doing the dimensionality reduction process, it’s important to choose which method we’d like to use to avoid valuable information missing."
  },
  {
    "objectID": "clustering/clustering1.html#footnotes",
    "href": "clustering/clustering1.html#footnotes",
    "title": "Clustering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“K-Means Clustering.” Wikipedia, Wikimedia Foundation, 12 Oct. 2023, en.wikipedia.org/wiki/K-means_clustering.↩︎\n“DBSCAN.” Wikipedia, Wikimedia Foundation, 27 Oct. 2023, en.wikipedia.org/wiki/DBSCAN.↩︎\n“Hierarchical Clustering.” Wikipedia, Wikimedia Foundation, 10 Oct. 2023, en.wikipedia.org/wiki/Hierarchical_clustering.↩︎"
  },
  {
    "objectID": "decision_trees/tree.html",
    "href": "decision_trees/tree.html",
    "title": "Decision Tree & Random Forest",
    "section": "",
    "text": "A tree structure that describes instance classification is called a classification decision tree model. The nodes and directed edges create a decision tree. Inner nodes and leaf nodes are the two distinct types of nodes. Leaf nodes indicate a class, while internal nodes represent as a feature or attribute. A decision tree, additionally referred to as a binary or multinomial tree, is a predictive analytical model displayed as a tree structure.\nOne way to classify instances is to arrange them from a root node to a leaf node. The classifications to which the examples belong are called leaf nodes. Every node in the tree represents a test of an instance attribute, and every branch that follows the node represents a potential attribute value.\n\n\n\nCreate a randomized forest with a large number of decision trees that are unrelated to one another. When a new input sample enters the forest after it has been obtained, every decision tree in the forest makes an assessment to determine which category (for classification algorithms) the sample should belong to. It then determines which category is more frequently selected, allowing the sample to be predicted for that category. Decision trees, which are essentially a method of partitioning space into hyperplanes and halving the current space each time, make up a random forest. Basically Random Forest are composite by many decision tree.\n\n\n\nFor my dataset of Mobile phones, I used a decision tree to do the classification. Based on the mobile phone prices, the function ratings of the mobile phone, for example, the camera rating, display rating, battery rating, etc. Based on those data, the decision tree model should predict the label of whether the public is willing to buy or not. The predicted label was divided into binary classification: not willing, and very willing. The decision tree model will give the final result."
  },
  {
    "objectID": "decision_trees/tree.html#train-tree-model",
    "href": "decision_trees/tree.html#train-tree-model",
    "title": "Decision Tree & Random Forest",
    "section": "Train Tree Model",
    "text": "Train Tree Model\n\n#### INSERT CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(x_train, y_train)\n\n\n# INSERT CODE TO USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND TEST SET \nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n\nfrom sklearn.metrics import confusion_matrix\n\ndef confusion_plot(Y_test, Y_pred):\n    matrix = confusion_matrix(Y_test, Y_pred)\n    accuracy = accuracy_score(Y_test, Y_pred)\n    precision_1 = precision_score(Y_test, Y_pred, pos_label = 1)\n    recall_1 = recall_score(Y_test, Y_pred, pos_label = 1)\n    precision_0= precision_score(Y_test, Y_pred, pos_label = 0)\n    recall_0 = recall_score(Y_test, Y_pred, pos_label = 0)\n    print(\"ACCURACY: \", accuracy)\n    print(\"NEGATIVE RECALL (Y=0): \",recall_0 )\n    print(\"NEGATIVE PRECISION (Y=0):\",precision_0) \n    print(\"POSITIVE RECALL (Y=1):\",recall_1) \n    print(\"POSITIVE PRECISION (Y=1):\", precision_1)\n    print(matrix)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(matrix, annot=True, fmt='d', cmap='YlGnBu', cbar=True)\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.show()\n\n\nresult\n\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACy:  1.0\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[88  0]\n [ 0 34]]\n------TEST------\nACCURACy:  0.967741935483871\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 0.9375\nPOSITIVE RECALL (Y=1): 0.9375\nPOSITIVE PRECISION (Y=1): 1.0\n[[15  0]\n [ 1 15]]\n\n\n\n\n\n\n\n\n\n\nVisualize Tree\n\n# INSERT CODE TO WRITE A FUNCTION \"def plot_tree(model,X,Y)\" VISUALIZE THE DECISION TREE (see https://mljar.com/blog/visualize-decision-tree/ for an example)\nfrom sklearn import tree\ndef plot_tree(model,X,Y):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\nplot_tree(model,X,Y)\n\n\n\n\n\n\nAnalysis\nBased on the training result, ACCURACY: 1.0, NEGATIVE RECALL (Y=0): 1.0, NEGATIVE PRECISION (Y=0): 1.0, POSITIVE RECALL (Y=1): 1.0, POSITIVE PRECISION (Y=1): 1.0, The accuracy, negative recall, negative precision, positive recall, positive precision are all 100%, which shows the training result can 100% predict the label, which is an awesome performance. Thus the model for the training set perform well.\nBased on the test result: ACCURACY: 0.967741935483871, NEGATIVE RECALL (Y=0): 1.0, NEGATIVE PRECISION (Y=0): 0.9375, POSITIVE RECALL (Y=1): 0.9375, POSITIVE PRECISION (Y=1): 1.0, Accuracy is 0.9677 and accuracy measures the overall correctness of a classification model. 96.77% means the model is not 100% correctly predicted the data, which may needs a little imporve. The negative recall and precision is 1 which mean 100% right, which means it perform well. The native precision measures the accuracy of the model when it predicts the negative class (class 0), and the sore is 0.9375, which may needs improve. The positive recall measures the ability of the model to correctly identify instances belonging to the positive class (class 1), and the score is 0.9375, so this may needs a little imporvement as well.\nThus for better fit in the model, I did a hyper-parameter Turning to see if there’s better model paramter to fit in."
  },
  {
    "objectID": "decision_trees/tree.html#result",
    "href": "decision_trees/tree.html#result",
    "title": "label modfication to binary",
    "section": "result",
    "text": "result\n\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACy:  1.0\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[88  0]\n [ 0 34]]\n------TEST------\nACCURACy:  0.967741935483871\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 0.9375\nPOSITIVE RECALL (Y=1): 0.9375\nPOSITIVE PRECISION (Y=1): 1.0\n[[15  0]\n [ 1 15]]"
  },
  {
    "objectID": "decision_trees/tree.html#visualize-tree",
    "href": "decision_trees/tree.html#visualize-tree",
    "title": "label modfication to binary",
    "section": "Visualize Tree",
    "text": "Visualize Tree\n\n# INSERT CODE TO WRITE A FUNCTION \"def plot_tree(model,X,Y)\" VISUALIZE THE DECISION TREE (see https://mljar.com/blog/visualize-decision-tree/ for an example)\nfrom sklearn import tree\ndef plot_tree(model,X,Y):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model, \n                    filled=True)\n    plt.show()\nplot_tree(model,X,Y)"
  },
  {
    "objectID": "decision_trees/tree.html#train-optimal-model",
    "href": "decision_trees/tree.html#train-optimal-model",
    "title": "Decision Tree & Random Forest",
    "section": "Train optimal model",
    "text": "Train optimal model\n\n#### COMPLETE THE CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier(max_depth=3)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n\n# RUN THE FOLLOWING CODE TO EVALUATE YOUR MODEL\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\nplot_tree(model,X,Y)\n\n------TRAINING------\nACCURACy:  1.0\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[88  0]\n [ 0 34]]\n------TEST------\nACCURACy:  0.967741935483871\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 0.9375\nPOSITIVE RECALL (Y=1): 0.9375\nPOSITIVE PRECISION (Y=1): 1.0\n[[15  0]\n [ 1 15]]\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis\nBased on the training and test result, the optimal result is actually the result we had interpreted before. Thus when the the tree’s layers=3, the model reach the optimal status and give us the test score as shown: ACCURACY: 0.967741935483871, NEGATIVE RECALL (Y=0): 1.0, NEGATIVE PRECISION (Y=0): 0.9375, POSITIVE RECALL (Y=1): 0.9375, POSITIVE PRECISION (Y=1): 1.0,\nEven though the score are not all 100%, the accuracy, negative precision,and positive recall are all higher than 90%, they are good enough to prove the model perform well to predict the label."
  },
  {
    "objectID": "decision_trees/tree.html#split-data",
    "href": "decision_trees/tree.html#split-data",
    "title": "Decision Tree & Random Forest",
    "section": "Split data",
    "text": "Split data\n\n# INSERT CODE TO PARTITION THE DATASET INTO TRAINING AND TEST SETS\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\nprint(\"X TRAINING SHAPES:\",x_train.shape)\nprint(\"Y TRAINING SHAPES:\",y_train.shape)\nprint(\"X TEST SHAPES:\",x_test.shape)\nprint(\"Y TEST SHAPES:\",y_test.shape)\n\nX TRAINING SHAPES: (122, 6)\nY TRAINING SHAPES: (122,)\nX TEST SHAPES: (31, 6)\nY TEST SHAPES: (31,)"
  },
  {
    "objectID": "decision_trees/tree.html#random-forest-training",
    "href": "decision_trees/tree.html#random-forest-training",
    "title": "Decision Tree & Random Forest",
    "section": "Random Forest training",
    "text": "Random Forest training\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = RandomForestClassifier(random_state=42)\nmodel = model.fit(x_train, y_train)\n\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)"
  },
  {
    "objectID": "decision_trees/tree.html#result-1",
    "href": "decision_trees/tree.html#result-1",
    "title": "Decision Tree & Random Forest",
    "section": "Result",
    "text": "Result\n\n# RUN THE FOLLOWING CODE TO TEST YOUR FUNCTION \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACy:  1.0\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 1.0\n[[88  0]\n [ 0 34]]\n------TEST------\nACCURACy:  0.967741935483871\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 0.9375\nPOSITIVE RECALL (Y=1): 0.9375\nPOSITIVE PRECISION (Y=1): 1.0\n[[15  0]\n [ 1 15]]\n\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\ntree.plot_tree(model.estimators_[0],\n               filled = True);\nfig.savefig('rf_individualtree.png')\n\n\n\n\n\nAnalysis\nBased on the training result, ACCURACY: 1.0, NEGATIVE RECALL (Y=0): 1.0, NEGATIVE PRECISION (Y=0): 1.0, POSITIVE RECALL (Y=1): 1.0, POSITIVE PRECISION (Y=1): 1.0, The accuracy, negative recall, negative precision, positive recall, positive precision are all 100%, which shows the training result can 100% predict the label, which is an awesome performance. Thus the model for the training set perform well.\nBased on the test result: ACCURACY: 0.967741935483871, NEGATIVE RECALL (Y=0): 1.0, NEGATIVE PRECISION (Y=0): 0.9375, POSITIVE RECALL (Y=1): 0.9375, POSITIVE PRECISION (Y=1): 1.0, Accuracy is 0.9677 and accuracy measures the overall correctness of a classification model. 96.77% means the model is not 100% correctly predicted the data, which may needs a little imporve. The negative recall and precision is 1 which mean 100% right, which means it perform well. The native precision measures the accuracy of the model when it predicts the negative class (class 0), and the sore is 0.9375, which may needs improve. The positive recall measures the ability of the model to correctly identify instances belonging to the positive class (class 1), and the score is 0.9375, so this may needs a little imporvement as well.\nThus for better fit in the model, I did a hyper-parameter Turning to see if there’s better model paramter to fit in."
  },
  {
    "objectID": "decision_trees/tree.html#hyperparameter-tuning",
    "href": "decision_trees/tree.html#hyperparameter-tuning",
    "title": "Decision Tree & Random Forest",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\n# COMPLETE THE FOLLOWING CODE TO LOOP OVER POSSIBLE HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,10):\n    model = RandomForestClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer, accuracy_score(y_train, yp_train), recall_score(y_train, yp_train, pos_label = 0), recall_score(y_train, yp_train, pos_label = 1)])\n\n\n# INSERT CODE TO GENERATE THE THREE PLOTS BELOW (SEE EXPECTED OUTPUT FOR EXAMPLE)\n\n# NOTE: THERE IS A TYPO IN THE THIRD PLOT, IT SHOULD BE RECALL IN THE Y-AXIS LABEL NOT ACCURACY\ntrain_df = pd.DataFrame(train_results, columns = ['max_depth', 'accuracy', 'recall_negative', 'recall_positive'])\ntest_df = pd.DataFrame(test_results, columns = ['max_depth', 'accuracy', 'recall_negative', 'recall_positive'])\n\nplt.plot(train_df['max_depth'], train_df['accuracy'], 'o-', color = 'blue', label = 'Train Accuracy')\nplt.plot(test_df['max_depth'], test_df['accuracy'], 'o-', color = 'red', label = 'Test Accuracy')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Accuracy (Y = 0): Training (blue) and Test (red)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nplt.plot(train_df['max_depth'], train_df['recall_negative'], 'o-', color = 'blue', label = 'Train Negative Recall')\nplt.plot(test_df['max_depth'], test_df['recall_negative'], 'o-', color = 'red', label = 'Test Negative Recall')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y = 0): Training (blue) and Test (red)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nplt.plot(train_df['max_depth'], train_df['recall_positive'], 'o-', color = 'blue', label = 'Train Positive Recall')\nplt.plot(test_df['max_depth'], test_df['recall_positive'], 'o-', color = 'red', label = 'Test Positive Recall')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y = 0): Training (blue) and Test (red)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn the Hype-parameter above, I let the model iterate between different depth of decision tree and then use the random forest model to fit in and try to find the optimal layers of decision tree and then form the which gives the highest accuracy and recall score, and this shows the optimal paramater for this random forest. Based on the graph 1 above, we can see that when the layer reach 2 both the train and test accuracy can reach 100% and the score line become flat, even when the layers numbers become higher, the model performance is not improving. Same situation appear in the both recall score plot. So, after depth=2, the model may become a little overfitting. Thus, we can conclude that when the depth=2, the model is optimal. Next we will use this optimal depth."
  },
  {
    "objectID": "decision_trees/tree.html#train-optimal",
    "href": "decision_trees/tree.html#train-optimal",
    "title": "Decision Tree & Random Forest",
    "section": "Train Optimal",
    "text": "Train Optimal\n\n#### COMPLETE THE CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train \nfrom sklearn import tree\nmodel = RandomForestClassifier(max_depth=2)\nmodel = model.fit(x_train, y_train)\n\nyp_train=model.predict(x_train)\nyp_test=model.predict(x_test)\n\n\n# RUN THE FOLLOWING CODE TO EVALUATE YOUR MODEL\nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n\n------TRAINING------\nACCURACy:  0.9918032786885246\nNEGATIVE RECALL (Y=0):  0.9886363636363636\nNEGATIVE PRECISION (Y=0): 1.0\nPOSITIVE RECALL (Y=1): 1.0\nPOSITIVE PRECISION (Y=1): 0.9714285714285714\n[[87  1]\n [ 0 34]]\n------TEST------\nACCURACy:  0.967741935483871\nNEGATIVE RECALL (Y=0):  1.0\nNEGATIVE PRECISION (Y=0): 0.9375\nPOSITIVE RECALL (Y=1): 0.9375\nPOSITIVE PRECISION (Y=1): 1.0\n[[15  0]\n [ 1 15]]\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\ntree.plot_tree(model.estimators_[0],\n               filled = True);\nfig.savefig('rf_individualtree.png')\n\n\n\n\n\nAnalysis\nThe accuracy score, negative recall and positive Precision score are not 100% for the test set. The accuracy score, negative precision, and positive precision score are not 100% of the training set as well. However, they all passed 90%, which are good enogh to prove the model perform well to predict the label."
  },
  {
    "objectID": "decision_trees/tree.html#split-train-set-and-test-set",
    "href": "decision_trees/tree.html#split-train-set-and-test-set",
    "title": "Decision Tree & Random Forest",
    "section": "split train set and test set",
    "text": "split train set and test set\n\n# INSERT CODE TO PARTITION THE DATASET INTO TRAINING AND TEST SETS\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\nprint(\"X TRAINING SHAPES:\",x_train.shape)\nprint(\"Y TRAINING SHAPES:\",y_train.shape)\nprint(\"X TEST SHAPES:\",x_test.shape)\nprint(\"Y TEST SHAPES:\",y_test.shape)\n\nX TRAINING SHAPES: (122, 6)\nY TRAINING SHAPES: (122,)\nX TEST SHAPES: (31, 6)\nY TEST SHAPES: (31,)"
  },
  {
    "objectID": "decision_trees/tree.html#hyper-parameter-turning",
    "href": "decision_trees/tree.html#hyper-parameter-turning",
    "title": "Decision Tree & Random Forest",
    "section": "Hyper-parameter Turning",
    "text": "Hyper-parameter Turning\n\n# COMPLETE THE FOLLOWING CODE TO LOOP OVER POSSIBLE HYPER-PARAMETERS VALUES\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,10):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0),recall_score(y_test, yp_test,pos_label=1)])\n    train_results.append([num_layer, accuracy_score(y_train, yp_train), recall_score(y_train, yp_train, pos_label = 0), recall_score(y_train, yp_train, pos_label = 1)])\n\n\n# INSERT CODE TO GENERATE THE THREE PLOTS BELOW (SEE EXPECTED OUTPUT FOR EXAMPLE)\n\n# NOTE: THERE IS A TYPO IN THE THIRD PLOT, IT SHOULD BE RECALL IN THE Y-AXIS LABEL NOT ACCURACY\ntrain_df = pd.DataFrame(train_results, columns = ['max_depth', 'accuracy', 'recall_negative', 'recall_positive'])\ntest_df = pd.DataFrame(test_results, columns = ['max_depth', 'accuracy', 'recall_negative', 'recall_positive'])\n\nplt.plot(train_df['max_depth'], train_df['accuracy'], 'o-', color = 'blue', label = 'Train Accuracy')\nplt.plot(test_df['max_depth'], test_df['accuracy'], 'o-', color = 'red', label = 'Test Accuracy')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Accuracy (Y = 0): Training (blue) and Test (red)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nplt.plot(train_df['max_depth'], train_df['recall_negative'], 'o-', color = 'blue', label = 'Train Negative Recall')\nplt.plot(test_df['max_depth'], test_df['recall_negative'], 'o-', color = 'red', label = 'Test Negative Recall')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y = 0): Training (blue) and Test (red)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nplt.plot(train_df['max_depth'], train_df['recall_positive'], 'o-', color = 'blue', label = 'Train Positive Recall')\nplt.plot(test_df['max_depth'], test_df['recall_positive'], 'o-', color = 'red', label = 'Test Positive Recall')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('Recall (Y = 0): Training (blue) and Test (red)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn the Hype-parameter above, I let the model iterate between different depth of decision tree and try to find the optimal layers of decision which gives the highest accuracy and recall score. Based on the graph 1 above, we can see that when the layers=3 both the train and test accuracy can reach 100%. When the layers numbers become higher, the model show a little overfitting. Based on the graph2, we also can see that when the layers=3 both the train and test accuracy can reach 100%. When the layers numbers become higher, the model’s performance did not improve. Thus, we can conclude that when the depth=3, the model is optimal. Next we will use this optimal depth."
  },
  {
    "objectID": "decision_trees/tree.html#decision-tree",
    "href": "decision_trees/tree.html#decision-tree",
    "title": "Decision Tree & Random Forest",
    "section": "",
    "text": "A tree structure that describes instance classification is called a classification decision tree model. The nodes and directed edges create a decision tree. Inner nodes and leaf nodes are the two distinct types of nodes. Leaf nodes indicate a class, while internal nodes represent as a feature or attribute. A decision tree, additionally referred to as a binary or multinomial tree, is a predictive analytical model displayed as a tree structure.\nOne way to classify instances is to arrange them from a root node to a leaf node. The classifications to which the examples belong are called leaf nodes. Every node in the tree represents a test of an instance attribute, and every branch that follows the node represents a potential attribute value."
  },
  {
    "objectID": "decision_trees/tree.html#random-forest",
    "href": "decision_trees/tree.html#random-forest",
    "title": "Decision Tree & Random Forest",
    "section": "",
    "text": "Create a randomized forest with a large number of decision trees that are unrelated to one another. When a new input sample enters the forest after it has been obtained, every decision tree in the forest makes an assessment to determine which category (for classification algorithms) the sample should belong to. It then determines which category is more frequently selected, allowing the sample to be predicted for that category. Decision trees, which are essentially a method of partitioning space into hyperplanes and halving the current space each time, make up a random forest. Basically Random Forest are composite by many decision tree."
  },
  {
    "objectID": "decision_trees/tree.html#apply-to-the-dataset",
    "href": "decision_trees/tree.html#apply-to-the-dataset",
    "title": "Decision Tree & Random Forest",
    "section": "",
    "text": "For my dataset of Mobile phones, I used a decision tree to do the classification. Based on the mobile phone prices, the function ratings of the mobile phone, for example, the camera rating, display rating, battery rating, etc. Based on those data, the decision tree model should predict the label of whether the public is willing to buy or not. The predicted label was divided into binary classification: not willing, and very willing. The decision tree model will give the final result."
  },
  {
    "objectID": "data_gathering/data_gathering.html#main-goal",
    "href": "data_gathering/data_gathering.html#main-goal",
    "title": "Data Gathering",
    "section": "",
    "text": "For this project, the main point is to find people’s attitudes towards Apple Products and evaluate what kind of mobile products would attract the public to purchase. Thus, I need to see both text data to reflect people’s attitudes toward Apple Products and record data to evaluate the products’ popularity and people’s willingness to buy. Thus, I need to search online to find either people’s comments about Apple Products or the news reports related to Apple products. Also, I need to find some datasets to evaluate what kinds of mobile products would be popular in the market so that when the new generation of Apple Products comes out, we can predict the popularity of the new products."
  },
  {
    "objectID": "data_gathering/data_gathering.html#dataset-finding",
    "href": "data_gathering/data_gathering.html#dataset-finding",
    "title": "Data Gathering",
    "section": "",
    "text": "News API: Gathering news’ reports to reflect the public attitudes towards Apple products.\nNews Webpage: Collect more news reports to reflect the public attitudes\nWeibo: Crawl comments from the social media to see the users’ reaction to the new Apple Products\nYahoo! Finance: collect Apple’s recent stock performance and this also can reflect the apple products’ popularity.\nMobile phone ratings: trying to find a dataset that have ratings related to all different functions of the mobile phones and what their popularity with the mobile phones."
  },
  {
    "objectID": "data_gathering/data_gathering.html#tools-using",
    "href": "data_gathering/data_gathering.html#tools-using",
    "title": "Data Gathering",
    "section": "",
    "text": "Python\nR\nNews API\nDownloading Dataset"
  },
  {
    "objectID": "data_gathering/data_gathering.html#data-gathering-1",
    "href": "data_gathering/data_gathering.html#data-gathering-1",
    "title": "Data Gathering",
    "section": "",
    "text": "Python, Web crawl\nDataset Collecrion: Weibo is a Chinese social platform that Chinese public sharing thoughts and post articles. There are 52 illion daily active users via that social media.I crawled data from Weibo to catch some hot posts about people’s reaction to the new publishion of Apple products. Most of the articles relected strong sentiments towards the products\nweibo data gathering code (python)\nraw data for weibo(python)\nsample raw data \nDataset Exaplaination: From the view of dataset, this is a text dataset and we can see the users’ ID and the publish dates and also the contents and commens(in Chinese).\n\n\n\n\n\nPython, News API\nDataset Collection: crawl data by using NewsAPI with Python and try to extract some key words from the news. The news API can help me to find the most updated articles related to the topics I am trying to search and based on the contens, I can analyze those text to see what’s the attitude of media towards the iPhone.\nNews data gathering code (python)\nraw data for newsAPI(python)\nsample raw data \nDataset Exaplaination: As shown above, this is a text dataset and we can read all the articles that News API collected.\n\n\n\n\n\nR, Rvest\nDataset Collection: Revest is also an useful tool to help up to crawl conetent from the website we want. I used Rvest to craw text with R and get content from the news website I selected.\nNews webpage data gathering code (R)\nraw data for news webpage(R)\nsample raw data \nDataset Exaplaination: as shown in above, as shown above, this is also a text dataset and we can read all the articles that Rvest collected.\n\n\n\n\n\nDownloading from Yahoo! Finance\nDataset Collection: Stock price and trend also can reflect the company’s popularity and how the public’s attitude towards the company. Thus I downloaded the Apple stock’s recent stock price and trying to analyze its stock trend.\nDataset source: Apple Stock in Yahoo! Finance\nsample raw data \nDataset Explaination: As shown in the dataset, this is a record dataset. We can see there’s the date, Openprice, highest price, lowest Price, close price, and trading volume. Based on this dataset, for nect next step we can analyze its stock prices changes.\n\n\n\n\n\nCollecting from Dxomark\nDataset collection: The dataset that evaluate the functions for a mobile phone can help me to identify the public’s buying willingness to the product. Thus I collected the dataset from the DXOMARK website that reflect the device’s performance and the quality of the user experience.\nDataset source: DXOMARK smartphone reviews\nsample raw data \nDataset Explaination: As shown in the dataset, this is a record dataset. The dataset has its ranking and devices’ name, its Launch price, Launche date, camera rating, selfie rating, audio rating, display rating and battery ratings. All those ratings are important factors that impact the customers’ buying inetent. Thus this dataset will help me to predict wether a smartphone product will be popular based on the ratings.\n\n\n\n\n\nKaggle dataset\nDataset collection: This dataset is both record and text data. I download it from the kaggle website. This dataset can help me better understand people’s reaction and reviews towards apple products.\nDataset source: Kaggles\nsample raw data \nDataset Explaination: the dataset includs the customers’ reviews and ratings from Amazon for iphone 11. The text contained people’s reviews and comments. The record data are related to people’s ratings, helpful counts, review country, etc. For this dataset, I will simply use for the data explortion process for better understand this topic.\n\n\n\n\nIn this part, I collected 6 different datasets that related to my research topic, Apple Products’ Popularity. The datasets are including text data and record data. The methods I used to collect data are using news API, crawling data with Python, Rvest with R, and downloading related data directly from the website.\nNext step I will clean and modified the data as need for preparing for the future model training."
  },
  {
    "objectID": "index.html#topics-introduction",
    "href": "index.html#topics-introduction",
    "title": "DSAN-5000: Introduction",
    "section": "Topics Introduction",
    "text": "Topics Introduction\n\nSummary\nOver the decades, Apple’s products are always the most popular products in the markets. However, with more and more new products introduced to the market. Are Apple products, especially iphone, are still popular in the market? How people’s reaction to this brand? Postive or negative? How does Apple perform compared with its competitors? How can we predict the popularity of Apple’s new product? In this project, we can dig into those problems, with the full data science cycle, we can use data to answer those questions.\nImportance\nWhy should we focus on Apple? Since its launch in 2007, the iPhone has had a huge impact on both technology and culture. Since it has affected and influenced a generation, everyone is interested in learning about Apple’s recent performance. It is a valuable subject for study because of its influence on mobile technology, communication norms, and consumer behavior.\nWhy the reader should read this project\nUsers of iPhones may exhibit decisions and preferences that are indicative of larger patterns in consumer behavior. Through this research subject, users can select better items and observe how other consumers respond to Apple’s offerings.\nwhat work had done\nThe researchers had done many data collection, data preprocessing, and set up machine learning models to do the logistic regression, decision trees and random forest, etc. They also had done so many behavioral analysis to analyze the popularity trend for electronic products.\nwhat are the “different points of views”/interpretations in the literature\nSome literature emphasizes Apple’s commitment to innovation and high-quality products as a key factor in its popularity. This view holds that the company’s focus on design, user experience, and robust performance drives consumer preference. Some authors also discuss Apple’s cultural impact, suggesting that its products have become more than just technology but a part of modern culture.1\nWhat need to be explored in this topic:\n\nWe can explore how people evaluate an electronic products as a good products.\nWe can explore the stock market for the Apple Products and how it stock prices changes\nWe can collect customers’ review/news on Apple products, to see how everyone like apple’s products.\n\nGoals and Hypothesis\nThe purpose of this study is to determine whether Apple products are still in demand and to ascertain how the general public feels about the company’s latest offerings. In addition, I’d like to know what specifications a mobile phone should have in order to draw in customers. So that when the next generation of iPhones came out, I could use this model to predict its popularity, I should train some models that can predict a product’s popularity based on its functions."
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "DSAN-5000: Introduction",
    "section": "Next Steps:",
    "text": "Next Steps:\nTo complete this research topic, I need to find datasets and related information related to people’s reviews that reflect people’s actual reaction related to Apple brands and iPhone Products. Also, I need to find some datasets that can help me to research what kind of quality of functions that would attract a customer to purchase. Whether Apple products perform well in those functions area? After I collected the datasets, I need to clean them as needed and prepare for the future model use and data analysis process.\nAll the code can be found in the code tab and data can be found in the data tab. The followings are some process that I will do in this process, and each topic can be found in each tab.\n\nData gathering\nData cleaning\nData Exploration\nClustering\nDimensionality Reduction\nDecision Tree\nNaive Bayes\nARM"
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#main-goal",
    "href": "data_cleaning/data_cleaning.html#main-goal",
    "title": "Data Cleanning",
    "section": "",
    "text": "The purpose of data cleansing is to remove data noise points by eliminating erroneous, duplicate parts of the data. In this part, I will first do a statistical analysis to identify the dataset’s mean, variance, frequency, etc. This step lets us find the datasets’ noise points and clean them as needed. For the record data, most data cleaning processes will deal with the abnormal values, fill null values, remove missing values, etc. For the text data, most data cleaning processes will involve cleaning the stop words, removing white spaces, etc."
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#main-methods-for-data-cleaning",
    "href": "data_cleaning/data_cleaning.html#main-methods-for-data-cleaning",
    "title": "Data Cleanning",
    "section": "Main Methods For Data Cleaning",
    "text": "Main Methods For Data Cleaning\nRecord Data\n\nfill null values: There are always missing values in the record dataset. Thus, we need to decide whether we should remove the null values or fill the null values. We can often use the mean/median to fill the null value based on the dataset itself.\nremove duplicates: Usually, the dataset may have the same entries, and that will affect our future data analysis. Thus, we should remove them as needed in case they impact the results.\nhandling outliers: Some data points are obviously different than others thus, we need to think about whether that was recorded by mistake and unuseful to the data analysis and consider removing them.\nconvert data types: when the same data type does not match as it should be, we can convert them to the data types we need for future data analysis. Eg: string -&gt; int\n\nText Data\n\nRemove stop words: the stop words like “the, and, an” are not useful for the text analysis, so we need to remove them as needed.\nRemove special characters and punctuation: the special characters like “$ @ %” are not useful for the text analysis, so we need to remove them as needed.\nTokenization: we can split text data into sentences or words for better analyzation.\nStemming and Lemmatization: in English, the verbs or nouns usually have different forms but actually has the same means, so we need to do this step for better analyze the text data.\nRemove white spaces: Sometimes the white spaces or new lines will hinder the analysis process, so we should clean it"
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#data-cleaning",
    "href": "data_cleaning/data_cleaning.html#data-cleaning",
    "title": "Data Cleanning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nClean the NewsAPI Raw Data (gather by Python)\n\nPython\nExplaination: the iphone_content.txt is a text data, so I will clean the raw data with the motheods dealing with text data.\nraw data before cleaning \nnewsapi data cleaning code (python)\n\nCleaning Process\n\nimport needed packages: will use sklearn, json, re, pandas, etc\n\n\nRemove the special characters, lower cases, and remove white spaces: In this part, it will remove unnecessary punctations, spaces and also lower the capitalized cases.\n\n\n\nCode\ndef string_cleaner(input_string):\n    try: \n        '''\n        out=re.sub(r\"\"\"\n                    [,;@#&$-]+  # Accept one or more copies of punctuation\n                    \\ *           # plus zero or more copies of a space,\n                    \"\"\",\n                    \" \",          # and replace it with a single space\n                    input_string, flags=re.VERBOSE)\n        '''\n\n        #REPLACE SELECT CHARACTERS WITH NOTHING\n        out = re.sub('[’]+', '', input_string)\n\n        #ELIMINATE DUPLICATE WHITESPACES USING WILDCARDS\n        out = re.sub(r'\\s+', ' ', out)\n\n        #CONVERT TO LOWER CASE\n        out=out.lower()\n    except:\n        print(\"ERROR\")\n        out=''\n    return out\n\n\n\nLemitization: In this part, lemitization will reduce a word to its base or root form, which will help me analyze this dataset.\nstemming: in this part, stemming will remove affixes from words, which will help me analyze this dataset.\nword tokenize: in this part, tokenize will split the sentences to words, which will help me analyze this dataset.\nremove stopwords: in this part, removing stopwords will keep more valueble words for analysis.\nCountvectorizer: in this part, countvectorizer will help me to count how many times each word appears in the library, which will help in analyze the word frequency.\n\nAfter cleaning process, below is the after cleaning dataset\n\ncleaned data for newsapi(python)\noutput cleaning data \n\n\n\nclean the weibo hot post data (gather by Python)\n\nR\nExplaination: 微博清单.csv is a text data, I will clean this raw data with R, and reordered the dataset, dropped unnecessary columns/rows, and cleaned the text part by segmentation and clear stop words, etc.\nraw data before cleaning \nweibo data cleaning code (R)\n\nCleaning Process\n\nchange the column orders: in this step, reorganize the column will help me easier find more useful information\ncheck duplicated and drop duplicated rows: checked the duplicated rows but there isn’t any.\nreordered the dataframe by the descending order of the like numbers column: the like numbers reflect whether this comment is hot or not, so when the numbers are larger, the more influencial this comment will be.\ndrop unnecessary columns: the ‘id’ column is not useful for the data analysis, so I droped this column.\ndrop unnecessary rows: I deleted the posts’ likes numbers &lt;3, and keep popular post with post’s like number &gt;3, because they are more useful content\nsegment the content: segment the sentences into words\ncleaned stop words in the post content: removing stopwords will keep more valueble words for analysis.\n\nAfter cleaning process, below is the after cleaning dataset\n\ncleaned data for weibo(R)\noutput cleaning data \n\n\n\nClean the Mobile Phone Rating Dataset\n\nPython\nExplaination: mobile_phone_rating.csv is a record dataset, so I will clean the raw data with the motheods dealing with record data.\nraw data before cleaning \nmobile phone cleaning code (R)\n\nCleaning Process\n\nreplace ‘-’ and null to ‘0’: the raw dataset are all string type, and ‘-’ represents null values, change to 0 will be easier for future change column type\nchange the column’s type: change most columns that with numbers to int type and ‘Launch date’ column to date type.\nfill missing values: in this step, I choose to fill null values with each column’s average value\ncheck nulls: check if there’s still exist null values\ncreate label: create label based on the ‘Rank’ column for future machine learning training\ndrop unnecessray column: drop the ‘Rank’ column because it’s unnecessary for now\nshuffle the dataset: because the dataset was ordered by the rank, and I want the dataset not in a specific order for future model training\n\nAfter cleaning process, below is the after cleaning dataset * cleaned data for mobile phone rating\n\noutput cleaning data \n\n\n\nApple Stock Dataset\n\nThis dataset is clean and organized as needed, so skipped the cleaning process"
  },
  {
    "objectID": "data_cleaning/data_cleaning.html#next-step",
    "href": "data_cleaning/data_cleaning.html#next-step",
    "title": "Data Cleanning",
    "section": "Next step",
    "text": "Next step\nFor this part, I cleaned both record data and text data with necessary methods. All the datasets are cleaned as needed and ready for the exploratory data analysis and model training. Next step, I will do some EDA to familiar with our dataset and functions of them and this will help me to visualize the data as well."
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#dimensionality-introduction",
    "href": "dimensionality_reduction/reduction.html#dimensionality-introduction",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality introduction",
    "text": "Dimensionality introduction\nIn the field of machine learning, the dimensionality reduction refers to the use of some kind of projection method to project the data points in the original high-dimensional space to a lower dimensional space. By dimensionality reduction, we can reduce the error caused by redundant information and imporve the accuracy of recognition. Now, dimensionality has become part of data preparation process for future model training. Following are some main methods of dimensionality reduction.\nPrinciple Component Analysis (PCA) 1\nPrinciple Component Analysis is a commonly used linear dimensionality reduction method. One of its goals is to use fewer data dimensions so that most can represent the original dataset’s characteristics by using some linear projection from high-dimension data into low-dimension data in the mean while maximizing the variance of the data being projected. Here’s a simple example explaining this concept. For example, if you want to organize some photos of people, the two characteristics you really care about are race and gender. When trying to reorganize the photos, you should find what matters to you most, reorganize the photos by the order of characteristics you focus more on, and then create a new display of those photos. So, this is how PCA works. Following are some main processes we doing PCA:\n\nStandardization\nCovariance Matrix Computation\nEigenvectors and Eigenvalues\nChoosing Principal Components\nTransforming Data\n\nt-SNE\nT-distributed stochastic Neighbor Embedding is a non-linear dimensionality reduction method that is used to form high-dimensional data in a low-dimensional environment. We aim to reduce the amount of complicated, multi-dimensional data regarding neighboring points (the “neighborhood”) to a more manageable format akin to what a distribution would use to express it. Method: Using these data points, we’ll model a random walk. Moving toward a nearby site during this process is more likely than to occur at a distant one. Lastly, we locate points in a space that is smaller in dimension. The neighborhood pattern around these locations ought to be very similar to the initial high-dimensional neighborhood distribution. 2"
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#tools",
    "href": "dimensionality_reduction/reduction.html#tools",
    "title": "Dimensionality Reduction",
    "section": "Tools",
    "text": "Tools\nIn this part, the library will include json, numpy, pandas, matplotlib, scikit-learn, sklearn.manifold.TSNE, etc."
  },
  {
    "objectID": "conclusion/conclusion.html#project-goal",
    "href": "conclusion/conclusion.html#project-goal",
    "title": "Conclusion",
    "section": "",
    "text": "The goal for this project is identify whether Apple products are still popular in the market, and find out public’s reaction to Apples’ new products. Also, I want to find out what kind of of quality of a mobile phone should have that will attract the public to buy. Thus, I should train some models that can predict a product’s populariy based on its functions, so when iPhone’s new generation came out, I can use thid model to predict its popularity."
  },
  {
    "objectID": "conclusion/conclusion.html#data-analysis-result",
    "href": "conclusion/conclusion.html#data-analysis-result",
    "title": "Conclusion",
    "section": "",
    "text": "In this part, I collected 3 text datasets and 2 record datasets by using API, Web Crawling, downloading methods by using Python and R. Most text datasets are related to the public’s reviews and comments towards Apple products,and one of the record datasets are related to Apple stock, and another is related to the mobile phones’ function rating. All the datasets can help me to dig into this research topic.\nHere’s some sample raw datasets:\nraw text data \nraw record data \n\n\n\nIn this part, I did data cleanning process for the datasets that I needed for preparing for future data analysis and model training. For the record data, I did fill null values, remove duplicates, handling outliers, convert data types, etc cleaning process. For the text data, I did remove stop words, remove special characters and punctuation, tokenization, stemming and lemmatization, remove white spaces, etc cleaning process.\nHere’s some sample cleaned datasets:\ncleaned text data \ncleaned record data \n\n\n\nIn this part, I did some exploratory data analysis for the cleaned datasets for better knowing the datasets. I used visualization tools to help the audience know more about my project, which will include text clouds, statistic distribution, correlation heatmap, etc.\nHere are some examples: \nHere’s some key words that appeared most frequently in the news dataset that I collected. As we can see, most words are natural and positive towards apple products.\n\n\n\nopen price for apple stock\n\n\nFrom the Apple stock’s plot we can see from the past year, the stock price is in the increasing trend, which means more poeple are willing to buy Apple’s stock, which means Apple is still popular in the market.\n\n\n\nTop 50 Ranking Brands\n\n\nAs shown in the pie chart, in the Top 50 ranking phones, 20% are apples. Which means there are 10 iphones are ranked in top50. This also shows Apple is still one of the most popular brands in the market.\n\n\n\nIn this part, I used K-means, DBSAN, Hierarchical clustering methods to clustered the mobile phone ratings dataset. Based on the hyperparameter turning, here’s the optimal clusters results: K-means: 2 DBSCAN : 4 Agglomerative Hierarchy: 2 Meanshift : 4 Birch : 3\nBased on the analysis process, I think Agglomerative Hierarchy would the best method for this dataset, and the optimal clustering would be 2.\n\n\n\nAgglomerative Hierarchy Clustering\n\n\n\n\n\nIn this part, I used PCA and t-SNE method to reduce the dimension for better analyzing the mobile phone ratings dataset. Based on the visualization result, I believe PCA would be a better method for this dataset.\n\n\n\nPCA\n\n\nAs shown in the graph, the red line intersect the blue line at number of comonents around = 4. So, when the number of compnents is larger than 4 which is 5, we can make 95% variance expliained. There for we should reduce the number of components to 5."
  },
  {
    "objectID": "conclusion/conclusion.html#model-training",
    "href": "conclusion/conclusion.html#model-training",
    "title": "Conclusion",
    "section": "",
    "text": "In this part, I used decision tree and random forest to do the classification. I training the model based on the mobile phone prices, the function ratings of the mobile phones to predict the public’s the buying intent to the products. - Decision Tree The optimal levels are 3\n\n\n\nDecision tree\n\n\nAnd the final test showed the model performed well: ACCURACY: 0.967741935483871 NEGATIVE RECALL (Y=0): 1.0 NEGATIVE PRECISION (Y=0): 0.9375 POSITIVE RECALL (Y=1): 0.9375 POSITIVE PRECISION (Y=1): 1.0 \n\nRandom Forest The result for the random forst is similar to the decision tree, and I believe the decision tree is better predict the model. More content related to random forest can be found in the Decision Tree Tab.\n\n\n\n\nIn this part, I used Naive Bayes algorithmn predict the lebels for the mobile phone ratings dataset and also the news text dataset. I used feature selection of variance thredhold method with hyperparameter tuning for better training the model.\nFinal result showed the naive bayes predicted my record dataset well. \n\n\n\nConfusion Metrics\n\n\nFinal result also showed the naive bayes predicted my text dataset well.\n\n\n\nEvaluation Metrics\n\n\n\n\n\nConfusion Metrics\n\n\nMore analysis can find in the Naive Bayes tab."
  },
  {
    "objectID": "conclusion/conclusion.html#questions-answers",
    "href": "conclusion/conclusion.html#questions-answers",
    "title": "Conclusion",
    "section": "",
    "text": "Based on the research above, I belive I can explain all the questions listed in the introduction well:\n\nCan customer reviews and sentiment analysis on social media reflect the public’s reaction to the products?\nYes, based on the sentiment analysis and key words extraction, we can see the public’s reaction easily.\nHow does Apple’s stock performed? Apple stock performed well in the past year.\nWhat are the trends in iPhone stocks sale volume over the last decade? the sale volumn are not increasing that much which may be a good signal for the current shareholder.\nWhat kind of electronic products can be seem as a good product to buy? higher in price, higher in the selfie, audio, etc functions ratings.\nHow does the pricing of Apple products compare with competitors over time, and what is the impact on market share and consumer choice? Apple’s product are kind of higher than the other competitors’ products\nHow can we predict whether Apple’s products would be popular in the future? From its function ratings, we can predict whether the public willing to buy or not.\nCan we use machine learning to identify key factors that predict an Apple product’s popularity based on historical data? Yes, based on the correlation map, we can see the price and selfie function are highly corelated and impacted the products’ popularity\nWhat kind of models we can used to predict people’s attitude with reviews? we can use the naive bayes or decision tree model to predict\nCan decision tree analysis help in identifying whether a product is a successful mobile phone product? Yes, we can use decision tree actually is the best method to predict the label.\nHow can we eliminate the unnecessary feature for predicting the Apple’s products? We can use some feature selection methods, like variance thredhold to eliminate some features."
  },
  {
    "objectID": "conclusion/conclusion.html#conclusion-1",
    "href": "conclusion/conclusion.html#conclusion-1",
    "title": "Conclusion",
    "section": "",
    "text": "From this project, we can know more about the public’s review and reaction on apple products and how to predict a new generation iphone’s popularity. From all the research above, I can conclude that Apple products are still popular in the market, and people’s reviews are more in postive way. The Apple stock price also reflect the popularity of Apple brand. Also the ratings of apple products are still very high compare to other brands.\nThus, apple products are still very popular and worthy to buy based on its functionality ratings. We can also use those ratings based on machine learning models to predict the popularity of future products."
  },
  {
    "objectID": "eda/eda.html#main-goal",
    "href": "eda/eda.html#main-goal",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "In this part, I will use visualization tools to help the audience know more about my project, which will include text clouds, statistic distribution, correlation heatmap, etc. This will help the audience to understand the general partterns of the data, understand the analysis of the data."
  },
  {
    "objectID": "about_me/about.html#about-me",
    "href": "about_me/about.html#about-me",
    "title": "Xue Qin",
    "section": "About Me",
    "text": "About Me\nMy name is Xue Qin and I’m currently pursuing a Master’s degree in Data Science and Analytics at the prestigious Georgetown University. I was born in Nanchong, Sichuan Province, China and came to the United States since high school.\nI graduated from the University of Minnesota, Twin Cities with a major in Finance and minors in Computer Science and German. I had past internships working in the Industrial Security Company, China Railway Investment Company, Tao Capital, and Sichuan Port and Shipping Group. Most of my work is related to business analysis, investment analysis, and data analysis with my programming skills in Python, R, Sql, etc.\nI have a strong interest in data science because it combines my love of solving problems with the ability of data to spur creativity and inform important choices. I get excited when I find hidden patterns and insights in complex datasets since each one has the power to influence and transform entire sectors. I’ve always been fascinated by how data science integrates many fields like computer science, statistics, and domain knowledge to produce useful findings. My interest has been stoked by the possibility of promoting technical advancement and using data-driven solutions to have a real impact on society. I am drawn to the field’s dynamic character because it requires ongoing learning and technology adaptation as part of the process. For me, data science is more than a career path; it is a commitment to lifelong learning and a way to fulfill my curiosity about the world around me through data."
  },
  {
    "objectID": "about_me/about.html#education-background",
    "href": "about_me/about.html#education-background",
    "title": "Xue Qin",
    "section": "Education Background",
    "text": "Education Background\n\n2019 - 2023: University of Minnesota, Twin Cities| Finance (major)/ Computer Science (minor) / German (minor)\n2023 - current: Georgetown University | Data Science and Analytics\n\n\n\n\nAcademic Interest\n\nMachine Learning Algorithms\nData Visualization\nHTML design\nNature Language Processing\n\n\n\nHobby\n\nPhotography\nPainting\nReading"
  },
  {
    "objectID": "about_me/about.html#academic-interest",
    "href": "about_me/about.html#academic-interest",
    "title": "Xue Qin",
    "section": "Academic Interest",
    "text": "Academic Interest\n\nMachine Learning Algorithms\nData Visualization\nHTML design\nNature Language Processing"
  },
  {
    "objectID": "about_me/about.html",
    "href": "about_me/about.html",
    "title": "Xue Qin",
    "section": "",
    "text": "Confusion Metrics"
  },
  {
    "objectID": "about_me/about.html#hobby",
    "href": "about_me/about.html#hobby",
    "title": "Xue Qin",
    "section": "Hobby",
    "text": "Hobby\n\nPhotography\nPainting\nReading"
  },
  {
    "objectID": "about_me/about.html#contact-info",
    "href": "about_me/about.html#contact-info",
    "title": "Xue Qin",
    "section": "Contact Info",
    "text": "Contact Info\n\nEmail: xq68@georgetown.edu\nPhone: (612)987-2866\nLinkedIn: Xue Qin"
  },
  {
    "objectID": "about_me/about.html#academic-interest-1",
    "href": "about_me/about.html#academic-interest-1",
    "title": "Xue Qin",
    "section": "Academic Interest",
    "text": "Academic Interest\n\nMachine Learning Algorithms\nData Visualization\nHTML design\nNature Language Processing"
  },
  {
    "objectID": "about_me/about.html#section",
    "href": "about_me/about.html#section",
    "title": "Xue Qin",
    "section": "",
    "text": "Photography\nPainting\nReading"
  },
  {
    "objectID": "dimensionality_reduction/reduction.html#footnotes",
    "href": "dimensionality_reduction/reduction.html#footnotes",
    "title": "Dimensionality Reduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDSAN 5000 week-10 https://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/slides/dimensionality-reduction/notes.html#t-distributed-stochastic-neighbor-embedding.↩︎\nDSAN 5000 week-10 https://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/slides/dimensionality-reduction/notes.html#t-distributed-stochastic-neighbor-embedding.↩︎\nDSAN 5000 week-10 https://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/slides/dimensionality-reduction/notes.html#t-distributed-stochastic-neighbor-embedding.↩︎"
  },
  {
    "objectID": "ARM/ARM.html#introduction",
    "href": "ARM/ARM.html#introduction",
    "title": "ARM",
    "section": "",
    "text": "Association rule mining (ARM) can be used in market analysis to determine what products, given their previous purchases, consumers will also buy. In order to examine consumer purchasing patterns For example, the ARM test can be run on an e-commerce dataset containing electronic transactions of various products by customers. i.e., people are more likely to buy a particular product later on if they purchase it first.\nThe antecedents are the feature data X. X is the description of the product that the customer purchased in this collection. Finding consequents (Y) is the first step in determining how likely it is that a consumer will buy product X and then purchase product Y based on their confidence and lift."
  },
  {
    "objectID": "ARM/ARM.html#theory",
    "href": "ARM/ARM.html#theory",
    "title": "ARM",
    "section": "Theory",
    "text": "Theory\nAn unsupervised rule-based machine learning technique called association rule mining (ARM) is used to find relationships between variables in big databases. The aim of ARM is to identify rules that will forecast the occurrence of an item based on the occurrences of other items in the training data, given a collection of transactions (training data).\nAn antecedent (if) and a consequent (then) comprise an association rule. Something that can be found in data is called an antecedent, and something that can be found in connection with the antecedent is called a consequent. Take A and B as itemsets, for instance. An association rule would be A =&gt; B.\n3 important metrics in ARM:\n\nSupport : (supp(X,Y)) As an indicator of how frequently the itemsets X and Y occur together in relation to all other transactions. For example, P(X∩Y)=(total number of transactions) / (number of transactions including X and Y).The minimum (support) criterion is used to eliminate samples that do not have sufficient support to be considered significant.\nConfidence : (Conf(X , Y)) Confidence measures the frequency with which items in X and Y occur together in relation to X-containing transactions. conf(X =&gt; Y) = P(Y|X) = supp(X ∩ Y) / supp(X) = (number of transactions containing X and Y) / (number of transactions contain X), for example. X is the antecedent and Y is the consequent in this scenario. One of the best ways to highlight the relationship between specific elements in the dataset is to use confidence in association rule mining.\nLift Lifte is the ratio of the actual support to what would be predicted if X and Y were independent is known as the lift. Lift = 1: no relationship between X and Y Lift &gt; 1: positive relationship between X and Y Lift &lt; 1: negative relationship between X and Y\n\nPackages use:\n\nApriori\nEclat\nFP-Growth"
  },
  {
    "objectID": "ARM/ARM.html#main-goal",
    "href": "ARM/ARM.html#main-goal",
    "title": "ARM",
    "section": "Main Goal",
    "text": "Main Goal\nI want to use the Assiciation rule mining to explore the intresting annd frequent patterns and relationships in my text dataset. This can help me to see the keywords’ relationship between each other."
  },
  {
    "objectID": "ARM/ARM.html#read-and-clean-text-data",
    "href": "ARM/ARM.html#read-and-clean-text-data",
    "title": "ARM",
    "section": "Read and clean text data",
    "text": "Read and clean text data\n\n#USER PARAM\ninput_path          =   '../../../data/00-raw-data/iphone_content.txt'\ncompute_sentiment   =   True        \nsentiment           =   []          #average sentiment of each chunck of text \nave_window_size     =   250         #size of scanning window for moving average\n                    \n\n#OUTPUT FILE\noutput='transactions.txt'\nif os.path.exists(output): os.remove(output)\n\n#INITIALIZE\nlemmatizer  =   WordNetLemmatizer()\nps          =   PorterStemmer()\nsia         =   SentimentIntensityAnalyzer()\n\n#ADD MORE\nstopwords   =   stopwords.words('english')\nadd=['mr','mrs','wa','dr','said','back','could','one','looked','like','know','around','dont']\nfor sp in add: stopwords.append(sp)\n\ndef read_and_clean(path,START=0,STOP=-1):\n    global sentiment \n\n    #-----------------------\n    #INSERT CODE TO READ IN AS ONE BIG STING\n    #-----------------------\n    file= open(path,'rt',encoding='utf-8')\n    text= file.read().lower() # convert to lower case\n    file.close()\n\n    #REMOVE HEADER, AND NEW LINES\n    text=text.replace(\"'\",'') #wasn't --&gt; wasnt\n    #lines = text.splitlines(); text=''; \n    #lines=lines[START:STOP]    # mystring.replace('\\n', ' ')\n    #for line in lines: text=text+' '+line\n\n    #-----------------------\n    #INSERT CODE TO ONLY KEEP CHAR IN string.printable\n    #-----------------------\n    tmp=''\n    printable=set(string.printable)\n    for char in text:\n        if char in printable:\n            tmp+= char\n    text= tmp\n\n    #BREAK INTO CHUNKS (SENTANCES OR OTHERWISE)\n    sentences=nltk.tokenize.sent_tokenize(text)  #SENTENCES\n\n    print(\"NUMBER OF SENTENCES FOUND:\",len(sentences)); #print(sentences)\n\n    #CLEAN AND LEMMATIZE\n    keep='0123456789abcdefghijklmnopqrstuvwxy';\n\n    new_sentences=[]; vocabulary=[]\n    for sentence in sentences:\n        new_sentence=''\n\n        # REBUILD LEMITIZED SENTENCE\n        for word in sentence.split():\n            \n            #ONLY KEEP CHAR IN \"keep\"\n            tmp2=''\n            for char in word: \n                if(char in keep): \n                    tmp2=tmp2+char\n                else:\n                    tmp2=tmp2+' '\n            word=tmp2\n\n            #-----------------------\n            # INSERT CODE TO LEMMATIZE THE WORDS\n            #-----------------------\n            word=tmp2\n            new_word=lemmatizer.lemmatize(word)\n\n            #REMOVE WHITE SPACES\n            new_word=new_word.replace(' ', '')\n\n            #BUILD NEW SENTANCE BACK UP\n            if( new_word not in stopwords):\n                if(new_sentence==''):\n                    new_sentence=new_word\n                else:\n                    new_sentence=new_sentence+','+new_word\n                if(new_word not in vocabulary): vocabulary.append(new_word)\n\n        #SAVE (LIST OF LISTS)       \n        new_sentences.append(new_sentence.split(\",\"))\n        \n        #SIA\n        if(compute_sentiment):\n            #-----------------------\n            # INSERT CODE TO USE NLTK TO DO SENTIMENT ANALYSIS \n            #-----------------------\n            text1=new_sentence.replace(',',' ')\n            score=sia.polarity_scores(text1)\n            sentiment.append([score['neg'],score['neu'],score['pos'],score['compound']])\n            \n        #SAVE SENTANCE TO OUTPUT FILE\n        if(len(new_sentence.split(','))&gt;2):\n            f = open(output, \"a\")\n            f.write(new_sentence+\"\\n\")\n            f.close()\n\n    sentiment=np.array(sentiment)\n    print(\"TOTAL AVERAGE SENTEMENT:\",np.mean(sentiment,axis=0))\n    print(\"VOCAB LENGTH\",len(vocabulary))\n    return new_sentences\n\ntransactions=read_and_clean(input_path,400,-400)\nprint(transactions[0:5])\n\nNUMBER OF SENTENCES FOUND: 107\nTOTAL AVERAGE SENTEMENT: [0.03593458 0.84782243 0.11621495 0.27573738]\nVOCAB LENGTH 1303\n[['best', 'premium', 'case', 'iphone', '15', 'iphone', '15', 'prostarting', 'year', 'apple', 'longer', 'make', 'signature', 'leather', 'case', 'iphones'], ['instead', 'company', 'launched', 'fabricbased', 'alternative', 'called', 'finewoven'], ['case', 'still', 'cost', '59', 'based', 'reviews', 'range', 'okay', 'terrible'], ['neapple', 'discontinues', 'iphone', '13', 'mini', 'last', 'small', 'phone', 'worth', 'owningthe', 'writing', 'wall', 'iphone', 'mini', 'series', 'last', 'year', 'apple', 'went', 'bigger', 'plus', 'sie', 'iphone', '14', 'instead', 'mini', 'model', 'offered', 'iphone', '12', '13', 'apple', 'ha', 'discontinued', 'iphone', '13', 'mini', 'entirely', 'tough', 'blow', 'fhow', 'preorder', 'iphone', '15', 'launch', 'daynow', 'apple', 'wonderlust', 'event', 'officially', 'four', 'model', 'iphone', '15', 'series', 'soon', 'able', 'preorder'], ['youre', 'interested', 'picking', 'iphone', '15', '15', 'plus', 'splurging', '15', 'pros', 'make', 'sure', 'order', 'youapple', 'iphone', '15', 'iphone', '15', 'pro', 'specs', 'price', 'release', 'datesay', 'goodbye', 'mute', 'switch', 'lightning', 'port']]\n\n\n\nAnalysis\nIn this part, I recleaned the text data with the news content that collected by the NewsAPI. I cleared out the stopwords, and cleared ou the special characters and punctuations. Than I created the transaction as needed.\n\n# INSERT CODE TO RE-FORMAT THE APRIORI OUTPUT INTO A PANDAS DATA-FRAME WITH COLUMNS \"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"\ndef reformat_results(results):\n    keep=[]\n    for i in range(0,len(results)):\n        for j in range(0,len(list(results[i]))):\n            if j&gt;1:\n                for k in range(0,len(list(results[i][j]))):\n                    if len(results[i][j][k][0]) != 0:\n                        rhs= list(results[i][j][k][0])\n                        lhs= list(results[i][j][k][1])\n                        conf= float(results[i][j][k][2])\n                        lift= float(results[i][j][k][3])\n                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n            if j == 1:\n                supp=results[i][j]\n    return pd.DataFrame(keep, columns=[\"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"])\n\n\ndef convert_to_network(df):\n    print(df)\n\n    #BUILD GRAPH\n    G = nx.DiGraph()  # DIRECTED\n    for row in df.iterrows():\n        # for column in df.columns:\n        lhs=\"_\".join(row[1][0])\n        rhs=\"_\".join(row[1][1])\n        conf=row[1][3]; #print(conf)\n        if(lhs not in G.nodes): \n            G.add_node(lhs)\n        if(rhs not in G.nodes): \n            G.add_node(rhs)\n\n        edge=(lhs,rhs)\n        if edge not in G.edges:\n            G.add_edge(lhs, rhs, weight=conf)\n\n    # print(G.nodes)\n    # print(G.edges)\n    return G\n\n\ndef plot_network(G):\n    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n    pos=nx.random_layout(G)\n\n    #GENERATE PLOT\n    fig, ax = plt.subplots()\n    fig.set_size_inches(15, 15)\n\n    #assign colors based on attributes\n    weights_e   = [G[u][v]['weight'] for u,v in G.edges()]\n\n    #SAMPLE CMAP FOR COLORS \n    cmap=plt.cm.get_cmap('Blues')\n    colors_e    = [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n\n    #PLOT\n    nx.draw(\n    G,\n    edgecolors=\"black\",\n    edge_color=colors_e,\n    node_size=2000,\n    linewidths=2,\n    font_size=8,\n    font_color=\"white\",\n    font_weight=\"bold\",\n    width=weights_e,\n    with_labels=True,\n    pos=pos,\n    ax=ax\n    )\n    ax.set(title='Dracula')\n    plt.show()\n\n\n# TRAIN THE ARM MODEL USING THE \"apriori\" PACKAGE\nprint('Transactions:',pd.DataFrame(transactions))\nresults=list(apriori(transactions,min_support= 0.1,min_confidence=0.5,use_colnames=True))\nprint(len(results))\n\nTransactions:          0             1         2            3            4          5    \\\n0       best       premium      case       iphone           15     iphone   \n1    instead       company  launched  fabricbased  alternative     called   \n2       case         still      cost           59        based    reviews   \n3    neapple  discontinues    iphone           13         mini       last   \n4      youre    interested   picking       iphone           15         15   \n..       ...           ...       ...          ...          ...        ...   \n102    apple           say     every       iphone           15     iphone   \n103  article        taking      look          new         case  accessory   \n104   iphone            15     major        price         hike    rumored   \n105   iphone            15        15         plus     expected   maintain   \n106      new       setting        se         None         None       None   \n\n           6            7         8             9    ...   150   151   152  \\\n0           15  prostarting      year         apple  ...  None  None  None   \n1    finewoven         None      None          None  ...  None  None  None   \n2        range         okay  terrible          None  ...  None  None  None   \n3        small        phone     worth     owningthe  ...  None  None  None   \n4         plus    splurging        15          pros  ...  None  None  None   \n..         ...          ...       ...           ...  ...   ...   ...   ...   \n102         15          pro      case      launched  ...  None  None  None   \n103      apple           ha  launched        iphone  ...  None  None  None   \n104     iphone           15       pro     modelsthe  ...  None  None  None   \n105    pricing      similar   current  modelsiphone  ...  None  None  None   \n106       None         None      None          None  ...  None  None  None   \n\n      153   154   155   156   157   158   159  \n0    None  None  None  None  None  None  None  \n1    None  None  None  None  None  None  None  \n2    None  None  None  None  None  None  None  \n3    None  None  None  None  None  None  None  \n4    None  None  None  None  None  None  None  \n..    ...   ...   ...   ...   ...   ...   ...  \n102  None  None  None  None  None  None  None  \n103  None  None  None  None  None  None  None  \n104  None  None  None  None  None  None  None  \n105  None  None  None  None  None  None  None  \n106  None  None  None  None  None  None  None  \n\n[107 rows x 160 columns]\n79\n\n\n\n\nAnalysis\nIn this part I applied the Apriori Algorithm to find put the frequent itemset of words that appear together in this text dataset.\n\n# PLOT THE RESULTS AS A NETWORK-X OBJECT \npd_results=reformat_results(results)\nG=convert_to_network(pd_results)\nplot_network(G)\n\n                     rhs           lhs      supp      conf  supp x conf  \\\n0            [announced]          [15]  0.102804  0.846154     0.086988   \n1                   [15]       [apple]  0.308411  0.532258     0.164154   \n2                [apple]          [15]  0.308411  0.702128     0.216544   \n3              [company]          [15]  0.112150  0.600000     0.067290   \n4                [event]          [15]  0.177570  0.863636     0.153356   \n..                   ...           ...       ...       ...          ...   \n194       [iphone, usbc]     [15, pro]  0.102804  0.733333     0.075389   \n195          [pro, usbc]  [15, iphone]  0.102804  1.000000     0.102804   \n196   [15, iphone, usbc]         [pro]  0.102804  0.733333     0.075389   \n197      [15, pro, usbc]      [iphone]  0.102804  1.000000     0.102804   \n198  [iphone, pro, usbc]          [15]  0.102804  1.000000     0.102804   \n\n         lift  \n0    1.460298  \n1    1.211736  \n2    1.211736  \n3    1.035484  \n4    1.490469  \n..        ...  \n194  1.669504  \n195  1.725806  \n196  1.601361  \n197  1.445946  \n198  1.725806  \n\n[199 rows x 6 columns]\n\n\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_5145/467521208.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  lhs=\"_\".join(row[1][0])\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_5145/467521208.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  rhs=\"_\".join(row[1][1])\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_5145/467521208.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  conf=row[1][3]; #print(conf)\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_5145/183469868.py:13: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  cmap=plt.cm.get_cmap('Blues')\n\n\n\n\n\n\n\nAnalysis\nI set min_support= 0.1, min_confidence=0.5, because I tried several times with smaller min_support and the result is too large so it did not ran out any resuly, but with larger min_support, too little results came out. When min_support= 0.1, min_confidence=0.5, the length of result is 79. So i believe this is the optimal number.\nFrom the plot we can see thet the most connection are between iphone and its devices’ name. So, when people mentions Apple, they always mention apples’ other devices names."
  },
  {
    "objectID": "ARM/ARM.html#conclusion",
    "href": "ARM/ARM.html#conclusion",
    "title": "ARM",
    "section": "Conclusion",
    "text": "Conclusion\nIn this part, I did data cleaning for the text data, and then created the transaction. After that, I trained the model and created the network and used Apriori package to dentifying the frequent individual items. Based on those I get the conclusion that, when people mentions Apple, they always mention its devices names, and also some positive adjective words. Thus, people has an positive attitude towards Apple products,"
  },
  {
    "objectID": "ARM/ARM.html#imporvement",
    "href": "ARM/ARM.html#imporvement",
    "title": "ARM",
    "section": "Imporvement",
    "text": "Imporvement\nThe result did provide some sense about people’s attitude related to apple products. But if we want to find out more about the its most competitive compeitors, those text dataset did not mention any, we maybe find a textdataset with more information with Apple and its competitors can be better analyzing its potential competitors."
  },
  {
    "objectID": "ARM/ARM.html#codes",
    "href": "ARM/ARM.html#codes",
    "title": "ARM",
    "section": "Codes",
    "text": "Codes\n\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom apyori import apriori\nimport networkx as nx \n\n\nnltk.download('vader_lexicon')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('omw-1.4')\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/sherryqin/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/sherryqin/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/sherryqin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/sherryqin/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /Users/sherryqin/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n\n\nTrue\n\n\n\nSeparate the text data\n\nimport pandas as pd\nimport numpy as np\n\n\ndf=pd.read_csv('../../../data/00-raw-data/apple_iphone_11_reviews.csv')\ndf\n\n\n\n\n\n\n\n\nindex\nproduct\nhelpful_count\ntotal_comments\nurl\nreview_country\nreviewed_at\nreview_text\nreview_rating\nproduct_company\nprofile_name\nreview_title\n\n\n\n\n0\n0\nApple iPhone XR (64GB) - Black\n5,087 people found this helpful\n24\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2018-12-12\nNOTE:\n3.0 out of 5 stars\nApple\nSameer Patil\nWhich iPhone you should Purchase ? iPhone 8, X...\n\n\n1\n1\nApple iPhone XR (64GB) - Black\n2,822 people found this helpful\n6\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2018-11-17\nVery bad experience with this iPhone xr phone....\n1.0 out of 5 stars\nApple\nAmazon Customer\nDon't buy iPhone xr from Amazon.\n\n\n2\n2\nApple iPhone XR (64GB) - Black\n1,798 people found this helpful\n0\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2019-01-27\nAmazing phone with amazing camera coming from ...\n5.0 out of 5 stars\nApple\nA\nHappy with the purchase\n\n\n3\n3\nApple iPhone XR (64GB) - Black\n1,366 people found this helpful\n14\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2019-05-02\nSo I got the iPhone XR just today. The product...\n1.0 out of 5 stars\nApple\nShubham Dutta\nAmazon is not an apple authorised reseller. Pl...\n\n\n4\n4\nApple iPhone XR (64GB) - Black\n536 people found this helpful\n5\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2019-05-24\nI've been an android user all my life until I ...\n5.0 out of 5 stars\nApple\nNepuni Lokho\nExcellent Battery life and buttery smooth UI\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5005\n5005\nApple iPhone XR (64GB) - Black\n0\n0\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2019-11-13\nDhamaka\n4.0 out of 5 stars\nApple\nShreya\nDhamaka phone\n\n\n5006\n5006\nApple iPhone XR (64GB) - Black\n0\n0\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2019-11-15\nGoodbye\n4.0 out of 5 stars\nApple\nmurali hv\nGood\n\n\n5007\n5007\nApple iPhone XR (64GB) - Black\n0\n0\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2019-12-29\nNothing\n5.0 out of 5 stars\nApple\nManish\nFantabulous phone. Easy to use.\n\n\n5008\n5008\nApple iPhone XR (64GB) - Black\n0\n0\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2019-11-10\nSuperbb\n5.0 out of 5 stars\nApple\nbasil john p\nFantastic\n\n\n5009\n5009\nApple iPhone XR (64GB) - Black\n0\n0\nhttps://www.amazon.in/Apple-iPhone-XR-64GB-Bla...\nIndia\n2019-11-05\nNothing\n5.0 out of 5 stars\nApple\nAmazon Customer\nBest purchase\n\n\n\n\n5010 rows × 12 columns\n\n\n\n\nreview_col = df['review_text']\n\n\nwith open('../../../data/00-raw-data/review.txt', 'w') as file:\n    for item in review_col:\n        file.write(\"%s\\n\" % item)\n\n\n\nRead and clean text data\n\n#USER PARAM\ninput_path          =   '../../../data/00-raw-data/review.txt'\ncompute_sentiment   =   True        \nsentiment           =   []          #average sentiment of each chunck of text \nave_window_size     =   250         #size of scanning window for moving average\n                    \n\n#OUTPUT FILE\noutput='transactions.txt'\nif os.path.exists(output): os.remove(output)\n\n#INITIALIZE\nlemmatizer  =   WordNetLemmatizer()\nps          =   PorterStemmer()\nsia         =   SentimentIntensityAnalyzer()\n\n#ADD MORE\nstopwords   =   stopwords.words('english')\nadd=['mr','mrs','wa','dr','said','back','could','one','looked','like','know','around','dont']\nfor sp in add: stopwords.append(sp)\n\ndef read_and_clean(path,START=0,STOP=-1):\n    global sentiment \n\n    #-----------------------\n    #INSERT CODE TO READ IN AS ONE BIG STING\n    #-----------------------\n    file= open(path,'rt',encoding='utf-8')\n    text= file.read().lower() # convert to lower case\n    file.close()\n\n    #REMOVE HEADER, AND NEW LINES\n    text=text.replace(\"'\",'') #wasn't --&gt; wasnt\n    #lines = text.splitlines(); text=''; \n    #lines=lines[START:STOP]    # mystring.replace('\\n', ' ')\n    #for line in lines: text=text+' '+line\n\n    #-----------------------\n    #INSERT CODE TO ONLY KEEP CHAR IN string.printable\n    #-----------------------\n    tmp=''\n    printable=set(string.printable)\n    for char in text:\n        if char in printable:\n            tmp+= char\n    text= tmp\n\n    #BREAK INTO CHUNKS (SENTANCES OR OTHERWISE)\n    sentences=nltk.tokenize.sent_tokenize(text)  #SENTENCES\n\n    print(\"NUMBER OF SENTENCES FOUND:\",len(sentences)); #print(sentences)\n\n    #CLEAN AND LEMMATIZE\n    keep='0123456789abcdefghijklmnopqrstuvwxy';\n\n    new_sentences=[]; vocabulary=[]\n    for sentence in sentences:\n        new_sentence=''\n\n        # REBUILD LEMITIZED SENTENCE\n        for word in sentence.split():\n            \n            #ONLY KEEP CHAR IN \"keep\"\n            tmp2=''\n            for char in word: \n                if(char in keep): \n                    tmp2=tmp2+char\n                else:\n                    tmp2=tmp2+' '\n            word=tmp2\n\n            #-----------------------\n            # INSERT CODE TO LEMMATIZE THE WORDS\n            #-----------------------\n            word=tmp2\n            new_word=lemmatizer.lemmatize(word)\n\n            #REMOVE WHITE SPACES\n            new_word=new_word.replace(' ', '')\n\n            #BUILD NEW SENTANCE BACK UP\n            if( new_word not in stopwords):\n                if(new_sentence==''):\n                    new_sentence=new_word\n                else:\n                    new_sentence=new_sentence+','+new_word\n                if(new_word not in vocabulary): vocabulary.append(new_word)\n\n        #SAVE (LIST OF LISTS)       \n        new_sentences.append(new_sentence.split(\",\"))\n        \n        #SIA\n        if(compute_sentiment):\n            #-----------------------\n            # INSERT CODE TO USE NLTK TO DO SENTIMENT ANALYSIS \n            #-----------------------\n            text1=new_sentence.replace(',',' ')\n            score=sia.polarity_scores(text1)\n            sentiment.append([score['neg'],score['neu'],score['pos'],score['compound']])\n            \n        #SAVE SENTANCE TO OUTPUT FILE\n        if(len(new_sentence.split(','))&gt;2):\n            f = open(output, \"a\")\n            f.write(new_sentence+\"\\n\")\n            f.close()\n\n    sentiment=np.array(sentiment)\n    print(\"TOTAL AVERAGE SENTEMENT:\",np.mean(sentiment,axis=0))\n    print(\"VOCAB LENGTH\",len(vocabulary))\n    return new_sentences\n\ntransactions=read_and_clean(input_path,400,-400)\nprint(transactions[0:5])\n\nNUMBER OF SENTENCES FOUND: 3571\nTOTAL AVERAGE SENTEMENT: [0.05495659 0.59596556 0.34040045 0.3765529 ]\nVOCAB LENGTH 4108\n[['note', 'bad', 'experience', 'iphone', 'xr', 'phone'], ['camera', 'found', 'defective', 'non', 'functional'], ['amaon', 'careless'], ['havent', 'got', 'response', 'confirmation', 'replacing'], ['hanging', 'damaged', 'piece', 'worth', '91000', 'rupees']]\n\n\n\nAnalysis\nIn this part, I recleaned the text data with the news content that collected by the NewsAPI. I cleared out the stopwords, and cleared ou the special characters and punctuations. Than I created the transaction as needed.\n\n\n\nVisualize Sentiment\n\n\ndef moving_ave(y,w=100):\n    #-----------------------\n    # INSERT CODE TO COMPUTE THE MOVING AVERAGE OF A SIGNAL Y\n    #-----------------------\n    mask=np.ones((1,w))/w; mask=mask[0,:]\n    return np.convolve(y,mask,'same')\n\n# INSERT CODE TO VISUALIZE THE SENTIMENT ANALYSIS AS A TIME-SERIES (SEE PLOT FOR AN EXAMPLE)\nif compute_sentiment :\n    neg=moving_ave(sentiment[:,0],ave_window_size); \n    neg=(neg-np.mean(neg))/np.std(neg)\n\n    neu=moving_ave(sentiment[:,1],ave_window_size); \n    neu=(neu-np.mean(neu))/np.std(neu)\n\n    pos=moving_ave(sentiment[:,2],ave_window_size); \n    pos=(pos-np.mean(pos))/np.std(pos)\n    \n    cmpd=moving_ave(sentiment[:,3],ave_window_size); \n    cmpd=(cmpd-np.mean(cmpd))/np.std(cmpd)\n    \n\n    # plot\n    plt.figure(figsize=(8,7))\n    indx= np.linspace(0,len(sentiment),len(sentiment))\n    plt.plot(indx,neg,label='negative')\n    plt.plot(indx,pos,label='positive')\n\n    plt.legend(loc='upper left',fontsize=12)\n    plt.xlabel('text chunks: progression of book')\n    plt.ylabel('sentiment')\n    plt.show()\n\n\n\n\n\n\nRe-format output\n\n# RE-FORMAT THE APRIORI OUTPUT INTO A PANDAS DATA-FRAME WITH COLUMNS \"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"\ndef reformat_results(results):\n    keep=[]\n    for i in range(0,len(results)):\n        for j in range(0,len(list(results[i]))):\n            if j&gt;1:\n                for k in range(0,len(list(results[i][j]))):\n                    if len(results[i][j][k][0]) != 0:\n                        rhs= list(results[i][j][k][0])\n                        lhs= list(results[i][j][k][1])\n                        conf= float(results[i][j][k][2])\n                        lift= float(results[i][j][k][3])\n                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n            if j == 1:\n                supp=results[i][j]\n    return pd.DataFrame(keep, columns=[\"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"])\n\n\ndef convert_to_network(df):\n    print(df)\n\n    #BUILD GRAPH\n    G = nx.DiGraph()  # DIRECTED\n    for row in df.iterrows():\n        # for column in df.columns:\n        lhs=\"_\".join(row[1][0])\n        rhs=\"_\".join(row[1][1])\n        conf=row[1][3]; #print(conf)\n        if(lhs not in G.nodes): \n            G.add_node(lhs)\n        if(rhs not in G.nodes): \n            G.add_node(rhs)\n\n        edge=(lhs,rhs)\n        if edge not in G.edges:\n            G.add_edge(lhs, rhs, weight=conf)\n\n    # print(G.nodes)\n    # print(G.edges)\n    return G\n\n\ndef plot_network(G):\n    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n    pos=nx.random_layout(G)\n\n    #GENERATE PLOT\n    fig, ax = plt.subplots()\n    fig.set_size_inches(15, 15)\n\n    #assign colors based on attributes\n    weights_e   = [G[u][v]['weight'] for u,v in G.edges()]\n\n    #SAMPLE CMAP FOR COLORS \n    cmap=plt.cm.get_cmap('Blues')\n    colors_e    = [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n\n    #PLOT\n    nx.draw(\n    G,\n    edgecolors=\"black\",\n    edge_color=colors_e,\n    node_size=2000,\n    linewidths=2,\n    font_size=8,\n    font_color=\"white\",\n    font_weight=\"bold\",\n    width=weights_e,\n    with_labels=True,\n    pos=pos,\n    ax=ax\n    )\n    ax.set(title='Dracula')\n    plt.show()\n\n\n\nTrain ARM model\n\n# TRAIN THE ARM MODEL USING THE \"apriori\" PACKAGE\nprint('Transactions:',pd.DataFrame(transactions))\nresults=list(apriori(transactions,min_support= 0.05,min_confidence=0.05,use_colnames=True))\nprint(len(results))\n\nTransactions:            0          1           2             3           4          5    \\\n0         note        bad  experience        iphone          xr      phone   \n1       camera      found   defective           non  functional       None   \n2        amaon   careless        None          None        None       None   \n3       havent        got    response  confirmation   replacing       None   \n4      hanging    damaged       piece         worth       91000     rupees   \n...        ...        ...         ...           ...         ...        ...   \n3566      good  excellent   excellent          love   excellent  excellent   \n3567   overall      great     product         loved        good       nice   \n3568      nice     colour     feature      fabulous    exultant   fabulous   \n3569     worth       good        good          good        good        buy   \n3570  excelent       good        good          good        good       bill   \n\n          6     7         8          9    ...   117   118   119   120   121  \\\n0        None  None      None       None  ...  None  None  None  None  None   \n1        None  None      None       None  ...  None  None  None  None  None   \n2        None  None      None       None  ...  None  None  None  None  None   \n3        None  None      None       None  ...  None  None  None  None  None   \n4        None  None      None       None  ...  None  None  None  None  None   \n...       ...   ...       ...        ...  ...   ...   ...   ...   ...   ...   \n3566     good  best      deal  excellent  ...  None  None  None  None  None   \n3567  product  need   invoice       soft  ...  None  None  None  None  None   \n3568   amaing  None      None       None  ...  None  None  None  None  None   \n3569     fine    af      None       None  ...  None  None  None  None  None   \n3570  missing  good  defected      piece  ...  None  None  None  None  None   \n\n       122   123   124   125   126  \n0     None  None  None  None  None  \n1     None  None  None  None  None  \n2     None  None  None  None  None  \n3     None  None  None  None  None  \n4     None  None  None  None  None  \n...    ...   ...   ...   ...   ...  \n3566  None  None  None  None  None  \n3567  None  None  None  None  None  \n3568  None  None  None  None  None  \n3569  None  None  None  None  None  \n3570  None  None  None  None  None  \n\n[3571 rows x 127 columns]\n46\n\n\n\nAnalysis\nIn this part I applied the Apriori Algorithm to find put the frequent itemset of words that appear together in this text dataset.\n\n\n\nVisualize the results\n\n# PLOT THE RESULTS AS A NETWORK-X OBJECT \npd_results=reformat_results(results)\nG=convert_to_network(pd_results)\nplot_network(G)\n\n                 rhs               lhs      supp      conf  supp x conf  \\\n0                 []           [phone]  0.052086  0.394068     0.020526   \n1            [phone]                []  0.052086  0.174648     0.009097   \n2            [apple]           [phone]  0.051806  0.424312     0.021982   \n3            [phone]           [apple]  0.051806  0.173709     0.008999   \n4          [awesome]           [phone]  0.054327  0.579104     0.031461   \n5            [phone]         [awesome]  0.054327  0.182160     0.009896   \n6          [battery]          [camera]  0.059647  0.345219     0.020591   \n7           [camera]         [battery]  0.059647  0.434694     0.025928   \n8          [battery]            [good]  0.088211  0.510535     0.045035   \n9             [good]         [battery]  0.088211  0.391304     0.034517   \n10         [battery]          [iphone]  0.050126  0.290113     0.014542   \n11          [iphone]         [battery]  0.050126  0.251404     0.012602   \n12         [battery]            [life]  0.105013  0.607780     0.063825   \n13            [life]         [battery]  0.105013  0.942211     0.098944   \n14         [battery]           [phone]  0.071689  0.414911     0.029744   \n15           [phone]         [battery]  0.071689  0.240376     0.017232   \n16            [best]          [iphone]  0.050686  0.453634     0.022993   \n17          [iphone]            [best]  0.050686  0.254213     0.012885   \n18            [best]           [phone]  0.059087  0.528822     0.031247   \n19           [phone]            [best]  0.059087  0.198122     0.011706   \n20          [camera]            [good]  0.070849  0.516327     0.036581   \n21            [good]          [camera]  0.070849  0.314286     0.022267   \n22          [camera]           [phone]  0.054327  0.395918     0.021509   \n23           [phone]          [camera]  0.054327  0.182160     0.009896   \n24          [camera]         [quality]  0.055447  0.404082     0.022405   \n25         [quality]          [camera]  0.055447  0.607362     0.033676   \n26            [good]          [iphone]  0.063568  0.281988     0.017925   \n27          [iphone]            [good]  0.063568  0.318820     0.020267   \n28            [good]            [life]  0.063008  0.279503     0.017611   \n29            [life]            [good]  0.063008  0.565327     0.035620   \n30            [good]           [phone]  0.113414  0.503106     0.057059   \n31           [phone]            [good]  0.113414  0.380282     0.043129   \n32            [good]         [product]  0.080090  0.355280     0.028454   \n33         [product]            [good]  0.080090  0.493955     0.039561   \n34           [great]           [phone]  0.060487  0.622478     0.037652   \n35           [phone]           [great]  0.060487  0.202817     0.012268   \n36          [iphone]           [phone]  0.079250  0.397472     0.031499   \n37           [phone]          [iphone]  0.079250  0.265728     0.021059   \n38          [iphone]              [xr]  0.071409  0.358146     0.025575   \n39              [xr]          [iphone]  0.071409  0.730659     0.052175   \n40            [life]           [phone]  0.050966  0.457286     0.023306   \n41           [phone]            [life]  0.050966  0.170892     0.008710   \n42           [phone]         [product]  0.066368  0.222535     0.014769   \n43         [product]           [phone]  0.066368  0.409326     0.027166   \n44         [battery]      [life, good]  0.061047  0.353323     0.021569   \n45            [good]   [battery, life]  0.061047  0.270807     0.016532   \n46            [life]   [battery, good]  0.061047  0.547739     0.033438   \n47   [battery, good]            [life]  0.061047  0.692063     0.042249   \n48   [battery, life]            [good]  0.061047  0.581333     0.035489   \n49      [life, good]         [battery]  0.061047  0.968889     0.059148   \n50            [good]  [phone, product]  0.050126  0.222360     0.011146   \n51           [phone]   [good, product]  0.050126  0.168075     0.008425   \n52         [product]     [good, phone]  0.050126  0.309154     0.015497   \n53     [phone, good]         [product]  0.050126  0.441975     0.022154   \n54   [product, good]           [phone]  0.050126  0.625874     0.031373   \n55  [product, phone]            [good]  0.050126  0.755274     0.037859   \n\n        lift  \n0   1.321330  \n1   1.321330  \n2   1.422740  \n3   1.422740  \n4   1.941767  \n5   1.941767  \n6   2.515870  \n7   2.515870  \n8   2.264745  \n9   2.264745  \n10  1.455049  \n11  1.455049  \n12  5.453218  \n13  5.453218  \n14  1.391218  \n15  1.391218  \n16  2.275179  \n17  2.275179  \n18  1.773168  \n19  1.773168  \n20  2.290437  \n21  2.290437  \n22  1.327535  \n23  1.327535  \n24  4.426305  \n25  4.426305  \n26  1.414294  \n27  1.414294  \n28  2.507803  \n29  2.507803  \n30  1.686939  \n31  1.686939  \n32  2.191197  \n33  2.191197  \n34  2.087202  \n35  2.087202  \n36  1.332744  \n37  1.332744  \n38  3.664583  \n39  3.664583  \n40  1.533305  \n41  1.533305  \n42  1.372493  \n43  1.372493  \n44  5.607621  \n45  2.578809  \n46  6.209444  \n47  6.209444  \n48  2.578809  \n49  5.607621  \n50  3.350415  \n51  2.098588  \n52  2.725896  \n53  2.725896  \n54  2.098588  \n55  3.350415  \n\n\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_7293/467521208.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  lhs=\"_\".join(row[1][0])\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_7293/467521208.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  rhs=\"_\".join(row[1][1])\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_7293/467521208.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  conf=row[1][3]; #print(conf)\n/var/folders/j3/s32rl54j1f76nvbcx9mxg4yc0000gn/T/ipykernel_7293/183469868.py:13: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  cmap=plt.cm.get_cmap('Blues')\n\n\n\n\n\n\nAnalysis\nI set min_support= 0.05, min_confidence=0.05, because I tried several times with smaller min_support and the result is too large so it did not ran out any resuly, but with larger min_support, too little results came out. When min_support= 0.05, min_confidence=0.05, the length of result is 46. So i believe this is the optimal number.\nFrom the plot we can see thet the most connection are between iphone and its device model and the functions. Also there are many positive adjective that highly related to iphone and its functions. For example, “phone” are connected to “awesome” “best” “good_product” and “good_phone”, etc. Thus, we can actually see when people mention iphone or its devices and functions, the most adjective are positive. So, we can conclude that people are positive to the Apple products"
  },
  {
    "objectID": "ARM/ARM.html#theory1",
    "href": "ARM/ARM.html#theory1",
    "title": "ARM",
    "section": "Theory1",
    "text": "Theory1\nAn unsupervised rule-based machine learning technique called association rule mining (ARM) is used to find relationships between variables in big databases. The aim of ARM is to identify rules that will forecast the occurrence of an item based on the occurrences of other items in the training data, given a collection of transactions (training data).\nAn antecedent (if) and a consequent (then) comprise an association rule. Something that can be found in data is called an antecedent, and something that can be found in connection with the antecedent is called a consequent. Take A and B as itemsets, for instance. An association rule would be A =&gt; B.\n3 important metrics in ARM:\n\nSupport : (supp(X,Y)) As an indicator of how frequently the itemsets X and Y occur together in relation to all other transactions. For example, P(X∩Y)=(total number of transactions) / (number of transactions including X and Y).The minimum (support) criterion is used to eliminate samples that do not have sufficient support to be considered significant.\nConfidence : (Conf(X , Y)) Confidence measures the frequency with which items in X and Y occur together in relation to X-containing transactions. conf(X =&gt; Y) = P(Y|X) = supp(X ∩ Y) / supp(X) = (number of transactions containing X and Y) / (number of transactions contain X), for example. X is the antecedent and Y is the consequent in this scenario. One of the best ways to highlight the relationship between specific elements in the dataset is to use confidence in association rule mining.\nLift Lifte is the ratio of the actual support to what would be predicted if X and Y were independent is known as the lift. Lift = 1: no relationship between X and Y Lift &gt; 1: positive relationship between X and Y Lift &lt; 1: negative relationship between X and Y\n\nPackages use:\n\nApriori\nEclat\nFP-Growth"
  },
  {
    "objectID": "ARM/ARM.html#footnotes",
    "href": "ARM/ARM.html#footnotes",
    "title": "ARM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikimedia Foundation. (2023, October 10). Association rule learning. Wikipedia. https://en.wikipedia.org/wiki/Association_rule_learning↩︎"
  },
  {
    "objectID": "Naive_Bayes/Naive_Bayes.html#footnotes",
    "href": "Naive_Bayes/Naive_Bayes.html#footnotes",
    "title": "Introduction to Naive Bayes:",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikimedia Foundation. (2023b, November 20). Naive Bayes classifier. Wikipedia. https://en.wikipedia.org/wiki/Naive_Bayes_classifier↩︎"
  },
  {
    "objectID": "decision_trees/tree.html#footnotes",
    "href": "decision_trees/tree.html#footnotes",
    "title": "Decision Tree & Random Forest",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSharma, A. (2023, September 26). Random Forest vs decision tree: Which is right for you?. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-algorithm/#:~:text=A.,data%2C%20especially%20on%20regression%20tasks.↩︎"
  }
]