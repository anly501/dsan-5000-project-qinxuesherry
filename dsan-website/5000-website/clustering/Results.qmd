# Results Analysis

## different methods comparing
- Optimal Clusters
1. K-means: 2
2. DBSCAN : 4
3. Agglomerative Hierarchy: 2
4. Meanshift : 4
5. Birch : 3

- Analysis
I think the above methods that predict the clusters well are Agglomerative Hierarchy > Birch > DBSCAN > K-means > Meanshift
The best method would be Agglomerative Hierarchy for the following reasons.
1. It's easier for me to use because I do not need to identify the k clusters before using it. It automatically gives me the optimal result.
2. Since my dataset is not that big, using agglomerative is more effective.
3. It can also make me quickly see the hierarchy relationships with my dataset.
4. The final result of the optimal clusters is two, which makes sense because in my original dataset, I labeled them into 3 clusters, and it turned out that for the "nature" cluster, there are not that many datasets belonging to this cluster.
5. Compared to K-means, as shown in the graphs of K-means, the elbow could be clearer for me. Compared to DBSCAN and Meanshift's result, 4 clusters would make the clusters contain insufficient data points. 
6. After observations, I believe the higher of the price and the other function rating features would be separated into a cluster. And the lower of the price and the other function rating features would be separated into another cluster. 

# Conclusion
I believe the agglomerative Hierarchy algorithm is better to predict the clusters for my dataset, and the optimal clusters would be 2.

Thus, for my dataset it should be separated into clusters about people's willingness to buy a mobile phone, which is not willing or willing depending on the functions of the mobile phones. 

In conclusion, the functionalities and prices of mobile phones impacted people's buying intent. Thus, people are more interested in the phone with good function ratings and high prices. Also, people are less willing to buy a phone with lousy function ratings and low costs.



